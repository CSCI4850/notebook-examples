{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-Layer Networks\n",
    "\n",
    "We have covered some of the limitations of single layer neural networks in class, but they are still powerful learning systems that provide a good way to begin learning about how to build neural networks using the `Keras` and `TensorFlow` tools.\n",
    "\n",
    "So, let's see how we can load some data vectors in from a file, and learn something using a single-layer network!\n",
    "\n",
    "We will start with a data set for classifying different species of iris plants based on size measurements taken from their flowers. The original data set can be found in the University of California, Irvine Machine Learning Reposity, which is linked to on the course website, but I've modified it just slightly for our purposes in this course.\n",
    "\n",
    "First things first, let's grab the data to start working with it. For this, we need `pandas` and a URL for loading the data...\n",
    "\n",
    "You can copy the URL for the data from [here](https://www.cs.mtsu.edu/~jphillips/courses/CSCI4850-5850/public/iris-data.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pandas\n",
    "import pandas\n",
    "\n",
    "# Load numpy too... we'll need it soon.\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris data set\n",
    "# Note the header=None option...\n",
    "data = np.array(pandas.read_table(\"https://www.cs.mtsu.edu/~jphillips/courses/CSCI4850-5850/public/iris-data.txt\",\n",
    "                                  delim_whitespace=True,\n",
    "                                  header=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas will return a special data structure that is capable of handling tables of mixed data types (strings, integers, floating point, etc.). However, this data set contains only numeric information (integers and floating point), making it reasonable for conversion into a numpy array. If we had a data set with mixed data types, we would need to work with the `pandas` data structure more closely to convert the non-numeric parts into vector-based encodings. The  vector encodings could then be used to provide this information to a neural network. We will revisit data encoding strategies at a later time, but for now we will stick to numeric data.\n",
    "\n",
    "Let's take a quick look at the **shape** of this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 5)\n"
     ]
    }
   ],
   "source": [
    "# Shape information\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, data sets will be arranged with one row per example. So for this data, we can assume there are 150 examples here (each is a set of measurements from a particular iris flower). Let's take a _slice_ of the data to see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.1,  3.5,  1.4,  0.2,  0. ],\n",
       "       [ 4.9,  3. ,  1.4,  0.2,  0. ],\n",
       "       [ 4.7,  3.2,  1.3,  0.2,  0. ]])"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Slice just the first 3 examples\n",
    "data[0:3,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can notice here that we can use the `:` operator (`0:3`) to specify a list of rows that we would like to extract from the matrix. We can also specify the number of columns using a similar construct, but here we would like to look at **all** columns for these three rows. We just use the `:` operator alone to perform this operation. Any selection of items performed in this way is called a _slice_, and is useful for exploring large data sets or forming new arrays from subsets of other arrays.\n",
    "\n",
    "For this data set, each example consists of a vector of _four_ features, and a _class label_ (five items total). Each of the four feature values are _continuous_ and the class labels are _discrete_. We can explore the data a little using the `unique()` function from numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  2.])"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the last column only...\n",
    "# .. and find the discrete set of items\n",
    "# that it contains...\n",
    "np.unique(data[:,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.3  2.   1.   0.1]\n",
      "[ 7.9  4.4  6.9  2.5]\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the other columns, too.\n",
    "# First, the minimum and next the maximum...\n",
    "print(np.min(data[:,0:4],axis=0))\n",
    "print(np.max(data[:,0:4],axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `unique()` function allows you to see the range of discrete values in an array. The 5th column of the data set contains only 0, 1, or 2. These are the _class labels_ for the examples. From a practical standpoint, these are the three different species of iris that we are wanting to classify. If you are interested in **exactly** which species the 0, 1, and 2 represent, please take a look at the details about the data set on the UCI repository page - [Link](http://archive.ics.uci.edu/ml/datasets/Iris). Understanding what the class labels correspond to in the real world might be important for understanding what our network is trying to tell us, a proper vector encoding of the class labels alone is sufficient for training a neural network.\n",
    "\n",
    "The `min()` and `max()` functions allow us to explore the range for the four different measurements obtained from each flower. Each represents a measurement in centimeters for: sepal length, sepal width, petal length, and petal width. Again, this mapping becomes abstracted away since the neural network experiences the each flower as a vector of measurements.\n",
    "\n",
    "At this time for simplicity, we will stick to the length-4 vector encoding of each flower. That is, the **training data** for the network that we will be building will just be all 150 vectors, each of length 4. We will store these **input vectors** in the matrix, $\\boldsymbol{X}$:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input training vectors\n",
    "X = data[:,0:4]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\boldsymbol{X}$ now contains all of the features for the 150 flowers we are going to try to classify using our neural network.\n",
    "\n",
    "For the class labels, we will construct a set of **target vectors** that represent the human-labeled class assignments for the flowers. While we _could_ just let the network attempt to assign a 0, 1, or 2, there is more useful approach to take. This approach motivated by the idea of what kind of output a network should produce for a classification problem. In our case, if we left the class label vector as-is, the *target* vector for each flower would be a 1-dimensional vector (i.e. a single scalar value). Since we construct our network input and output _layers_ to have the same number of units as our input and target vectors, respectively, we would only use a single output unit. This _output unit_ would need to have activation function capable of representing the possible target values (0,1,2) which limits our choices. However, with a different **encoding** of the target vectors, this will no longer be an issue, and we will get some other benefits along the way.\n",
    "\n",
    "Let's look at this method for transforming the class labels in detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First, let's grab the class labels by themselves...\n",
    "labels = data[:,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the labels \"in-hand\" we will use some tools from the `keras` package (which relies on the `tensorflow` package) to create the encoding we would like to use. We will first load the tools, then create the set of target vectors, $\\boldsymbol{Y}$ which we will be using for training our classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Keras/Tensorflow\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 3)\n"
     ]
    }
   ],
   "source": [
    "# Convert the integer class labels to a\n",
    "# categorical or \"one-hot\" encoding...\n",
    "Y = keras.utils.to_categorical(labels,\n",
    "                               len(np.unique(labels)))\n",
    "\n",
    "# Encoded vector size?\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this operation, the integer class labels have been encoded into a 3-dimensional vector space. The `unique()` function was used to determine the number of unique integer labels in the vector, and the vector itself gets passed to the `to_categorical()` function to produce this encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "[ 1.  0.  0.]\n",
      "1.0\n",
      "[ 0.  1.  0.]\n",
      "2.0\n",
      "[ 0.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "# Some particular examples...\n",
    "print(labels[0])\n",
    "print(Y[0,:])\n",
    "\n",
    "print(labels[50])\n",
    "print(Y[50,:])\n",
    "\n",
    "print(labels[100])\n",
    "print(Y[100,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, I have pulled three examples to illustrate how the encoding scheme works. The `0` class label has now been mapped to the vector $[1,0,0]$, the label `1` has now been mapped to the vector $[0,1,0]$, and the label `2` has been mapped to the vector $[0,0,1]$. This is known as a categorical (or _one-hot_) encoding, and is a common way to represent discrete (i.e. integer) information to a neural network. In fact, it can sometimes be useful to perform a similary mapping for integer data that is provided as _input_ to a network as well, but we will save that for another day.\n",
    "\n",
    "More importantly, since we now have vectors consisting only of scalar values in the range $[0,1]$, all typical activation functions that we have studied so far could be used since they can all produce output values in that same range (well, technically the acceptable range would be $(0,1)$ for the sigmoid function, for example, but it doesn't really matter much at the moment).\n",
    "\n",
    "Now that we have a set of **input patterns**, $\\boldsymbol{X}$, and a set of **target patterns**, $\\boldsymbol{Y}$, we can use these vectors for training a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Single-Layer Network\n",
    "\n",
    "Now that we have data ready for training, we just need to construct a network to learn how to to classify irises. The `keras` package provides the tools needed to set up such networks very quickly and start training them.\n",
    "\n",
    "We will start by setting up a data structure that will contain our network, known as the _model_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting up a single-layer network\n",
    "model = keras.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the `Sequential()` model which makes the assumtion that we would like to build a _feed-forward_ neural network architecture, which is what we have been focusing on so far in class.\n",
    "\n",
    "Now that we have the container, let's create a single layer network. We do this by adding it to the model using the `add()` member function. However, we also need to specify the kind of layer we want to add, and _some_ of its details. For our purposes, we are interested in adding a single layer (really, the _output layer_). Remember, the input layer is rather simple in that it doesn't perform computation, and instead just holds input pattern data during training and prediction (passing data through the network). So, we just need to tell this _output layer_ that it will receive data from the input layer of a certain size. We also need to create all of the connection weights between the input and output layer units, but this is all taken care of for us by the `Dense()` function.\n",
    "\n",
    "In a nutshell, when making a layer using the `Sequential()` model, all weights leading _into_ that layer will also need to be specified. There are different ways of connecting layers together, but for now we will mainly focus on densely connected networks, where all units in the previous layer will be connected to all units in the layer we are creating. Again, the `Dense()` function provides all of the functionality that we need for this operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a densely connected layer of units\n",
    "# and specify the input layer size (note,\n",
    "# the input layer is assumed to be there,\n",
    "# which makes this a single-layer network!)\n",
    "\n",
    "# Input size - 4\n",
    "input_size = X.shape[1]\n",
    "\n",
    "# Output size - 3\n",
    "output_size = Y.shape[1]\n",
    "\n",
    "# We are using a sigmoid activation\n",
    "# function, AND the input_size was\n",
    "# provided within a python list []...\n",
    "model.add(keras.layers.Dense(output_size,\n",
    "                             activation='sigmoid',\n",
    "                             input_shape=[input_size]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lovely new neural network!\n",
    "\n",
    "You can use the `summary()` function to get glimpse into what keras tools have created for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 3)                 15        \n",
      "=================================================================\n",
      "Total params: 15\n",
      "Trainable params: 15\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network follows the conventions we have utilized in class for neural units. That is the neural units, weight matrices, and bias weights have all been created for us using the `keras` tools.\n",
    "\n",
    "For example, we have a 4x3 weight matrix (12 connection weights), and three output units each with a bias weight, $w_{o}$, (3 bias weights total). Hence, we have 15 total weights that can be changed during the learning process, and these are known as _trainable parameters_ in the `keras` framework.\n",
    "\n",
    "The output units utilize the weighted sum calculation we have discussed in class (net input) and we have also specified a _sigmoid_ activation function for output.\n",
    "\n",
    "However, the model isn't quite ready to go. At this point, `model` contains only a **template** for what we want the network to be. We need to `compile()` the network to create the tensorflow data structures that _actually_ compute the neural network. Let's do that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep the model for -learning-\n",
    "model.compile(loss=keras.losses.mse,\n",
    "              optimizer=keras.optimizers.SGD(lr=0.001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, learning requires some method for specifying how to update the weights via experience with the training data.\n",
    "\n",
    "We will use the _stochastic gradient descent_ to perform this operation, and we select this by setting `optimizer = keras.optimizers.SGD(lr=0.01)` (note, the learning rate, `lr`, setting). However, we need to select an _error function_ as well that we would like to minimize for the optimizer to know what to optimize.\n",
    "\n",
    "In the current literature, since not all functions for describing goodness are really metrics for measuring _error_, the more general term, _loss_, is often used. We will be using a _loss_ function that is very similar to the SSE function we studied in class, but here it's the Mean-Squared-Error (`loss=keras.losses.mse`). Overall, you can think about this as being similar to multiplying the SSE by some fraction (like we did to derive the delta rule) based on the number of training examples used in each batch for calculating the weight update.\n",
    "\n",
    "Finally, while we will use loss to optimize the weights in the network, a more intuitive metric of performance is added to the model as well: _accuracy_. While accuracy isn't something used for optimization, if we assume that the strongest ouput from the network (whichever of the three output units produces the highest value) is the network's _best guess_ at what the current iris example should be, then we can calculate the fraction of the iris patterns that it is classifying correctly. Thus, 0.0 accuracy would indicate that the network is classifying -none- of the examples properly, but 1.0 accuracy would indicate that the network is classifying -all- of the examples properly.\n",
    "\n",
    "There are other things that the `compile()` function takes care of for us, such as setting the weights to some reasonable starting values. For now, we will trust the `compile()` function to do this job. However, we can always catch a glimpse of what the current weight values are in the network if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.68086886, -0.49133822,  0.52303326],\n",
       "        [-0.56564546, -0.21872205, -0.46792731],\n",
       "        [ 0.68101645, -0.40122145,  0.25592816],\n",
       "        [ 0.2164191 , -0.5513292 , -0.2578485 ]], dtype=float32),\n",
       " array([ 0.,  0.,  0.], dtype=float32)]"
      ]
     },
     "execution_count": 651,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the bias and connection weights...\n",
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A python list is returned: the first element contains the 4x3 weight matrix, and the second element contains the 3-element vector of output unit bias weights. You can see that the connection weights are initialized to _small_ random values, and the bias weights are initially set to zero. We will explore other methods for initializing the weights in later assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Single-Layer Network\n",
    "\n",
    "Time to get training! First, select a batch size for the stochastic gradient update: the number of patterns experienced between weight updates. Second, choose the number of epochs (complete passes through the data) that you would like to peform. Third, select a certain fraction of the data that you would like to use for _validation_ of your training results (0.5 would mean the 50% of the data is not used for training, but instead used to test for _generalization_).\n",
    "\n",
    "We will utilize the `fit()` member function of our model for perfoming the training, which accepts these three options to control its behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 75 samples, validate on 75 samples\n",
      "Epoch 1/10\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4562 - acc: 0.6667 - val_loss: 0.5374 - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "75/75 [==============================] - 0s 243us/step - loss: 0.4550 - acc: 0.6667 - val_loss: 0.5372 - val_acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "75/75 [==============================] - 0s 238us/step - loss: 0.4539 - acc: 0.6667 - val_loss: 0.5369 - val_acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "75/75 [==============================] - 0s 276us/step - loss: 0.4528 - acc: 0.6667 - val_loss: 0.5367 - val_acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "75/75 [==============================] - 0s 245us/step - loss: 0.4516 - acc: 0.6667 - val_loss: 0.5364 - val_acc: 0.0000e+00\n",
      "Epoch 6/10\n",
      "75/75 [==============================] - 0s 203us/step - loss: 0.4505 - acc: 0.6667 - val_loss: 0.5362 - val_acc: 0.0000e+00\n",
      "Epoch 7/10\n",
      "75/75 [==============================] - 0s 240us/step - loss: 0.4493 - acc: 0.6667 - val_loss: 0.5359 - val_acc: 0.0000e+00\n",
      "Epoch 8/10\n",
      "75/75 [==============================] - 0s 233us/step - loss: 0.4480 - acc: 0.6667 - val_loss: 0.5357 - val_acc: 0.0000e+00\n",
      "Epoch 9/10\n",
      "75/75 [==============================] - 0s 238us/step - loss: 0.4468 - acc: 0.6667 - val_loss: 0.5354 - val_acc: 0.0000e+00\n",
      "Epoch 10/10\n",
      "75/75 [==============================] - 0s 254us/step - loss: 0.4456 - acc: 0.6667 - val_loss: 0.5351 - val_acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# Basic training parameters\n",
    "batch_size = 16\n",
    "epochs = 10\n",
    "validation_split = 0.5\n",
    "\n",
    "# Train the model and record the training\n",
    "# history for later examination\n",
    "history = model.fit(X, Y,\n",
    "          batch_size = batch_size,\n",
    "          epochs = epochs,\n",
    "          verbose = 1,\n",
    "          validation_split = validation_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that you will get some output for each epoch that you train the network, indicating progress through the training. You can turn **off** this output by using the `verbose=0` option at any time. This is sometimes useful since there are other ways that we can look at the training performance using the `history` data that came back from the fitting process.\n",
    "\n",
    "You can see that the loss values were **decreasing** (error was going down), even if **accuracy** wasn't necessarily increasing. To make this network perform better we could:\n",
    "1) Increase the number epochs used in the training process\n",
    "2) Increase the learning rate on the stochastic gradient optimizer\n",
    "3) Rebuild the network starting from our `model = keras.Sequential()` statement to initialize the weights at a better starting location in the weight space.\n",
    "3) Other things that we will explore at a later time (**don't use any other tricks for this assignment**)...\n",
    "\n",
    "Let's plot the history information for a moment to see what happened across training. This is just a graphical depiction of what happened during the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f614036da20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(1)  \n",
    "   \n",
    "# summarize history for accuracy  \n",
    "   \n",
    "plt.subplot(211)  \n",
    "plt.plot(history.history['acc'])  \n",
    "plt.plot(history.history['val_acc'])  \n",
    "plt.title('model accuracy')  \n",
    "plt.ylabel('accuracy')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='upper left')  \n",
    "   \n",
    "# summarize history for loss  \n",
    "   \n",
    "plt.subplot(212)  \n",
    "plt.plot(history.history['loss'])  \n",
    "plt.plot(history.history['val_loss'])  \n",
    "plt.title('model loss')  \n",
    "plt.ylabel('loss')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These graphical reports (after using `verbose=0`) will be very useful for completing this assignment. Use the code above as a template for constructing your graphs. Limited changes should be needed to complete this assignment.\n",
    "\n",
    "However, let's see how our network performs now on the entire data set. Now that we have trained the network, we can use the `evaluate()` method to determine this information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 63us/step\n",
      "Test loss: 0.489950377941\n",
      "Test accuracy: 0.333333333333\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X, Y, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, we are only getting 33% of the examples classified correctly! However, we can use one (or more) of the three suggested tricks above for improving the performance of the network. Unless you rebuild the model again from scratch, training can be carried over from previous `fit()` operations. So, if we ran fit again now for another 10 epochs, it would be 20 epochs total of training. However, the history information from the previous 10 epochs may be lost, so be sure to keep records of the training process or restart from scratch if you want to train for more epochs from the very beginning.\n",
    "\n",
    "Some models _also_ take a long time to evaluate, so the `verbose()` option is available to help determine how long this process takes to complete, but feel free to turn it off by setting it to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice!\n",
    "\n",
    "Now that you have experienced the *process* for creating a single-layer network, try adjusting the suggested parameters above to learn how to build a single-layer neural network to classify the Iris data set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
