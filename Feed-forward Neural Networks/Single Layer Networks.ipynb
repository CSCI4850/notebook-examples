{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-Layer Networks\n",
    "\n",
    "We have covered some of the limitations of single layer neural networks in class, but they are still powerful learning systems that provide a good way to begin learning about how to build neural networks using the `Keras` and `TensorFlow` tools. These tools used to be separate frameworks, but the `Keras` API has now been merged into `TensorFlow` and we will be using this merged API for our work here.\n",
    "\n",
    "So, let's see how we can load some data vectors in from a file, and learn something using a single-layer network!\n",
    "\n",
    "We will start with a data set for classifying different species of iris plants based on size measurements taken from their flowers. The original data set can be found in the University of California, Irvine Machine Learning Reposity, which is linked to on the course website, but I've modified it just slightly for our purposes in this course.\n",
    "\n",
    "First things first, let's grab the data to start working with it. For this, we need `pandas` and a URL for loading the data...\n",
    "\n",
    "You can copy the URL for the data from [here](https://www.cs.mtsu.edu/~jphillips/courses/CSCI4850-5850/public/iris-data.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pandas\n",
    "import pandas\n",
    "\n",
    "# Load numpy too... we'll need it soon.\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris data set\n",
    "# Note the header=None option...\n",
    "data = np.array(pandas.read_table(\n",
    "    \"https://www.cs.mtsu.edu/~jphillips/courses/CSCI4850-5850/public/iris-data.txt\",\n",
    "    delim_whitespace=True,\n",
    "    header=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas will return a special data structure that is capable of handling tables of mixed data types (strings, integers, floating point, etc.). However, the data set we just loaded contains only numeric information (integers and floating point), making it reasonable for conversion into a numpy array. If we had a data set with mixed data types, we would need to work with the `pandas` data structure more closely to convert the non-numeric parts into vector-based encodings. The  vector encodings could then be used to provide this information to a neural network. We will revisit data encoding strategies at a later time, but for now we will stick to numeric data.\n",
    "\n",
    "Let's take a quick look at the **shape** of this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 5)\n"
     ]
    }
   ],
   "source": [
    "# Shape information\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, data sets will be arranged with one row per example. So for this data, we can assume there are 150 examples here (each is a set of measurements from a particular iris flower). Let's take a _slice_ of the data to see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.8, 2.7, 3.9, 1.2, 1. ],\n",
       "       [6.9, 3.1, 5.4, 2.1, 2. ],\n",
       "       [7.7, 3. , 6.1, 2.3, 2. ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Slice just the first 3 examples\n",
    "data[0:3,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can notice here that we can use the `:` operator (`0:3`) to specify a list of rows that we would like to extract from the matrix. We can also specify the number of columns using a similar construct, but here we would like to look at **all** columns for these three rows. We just use the `:` operator alone to perform this operation. Any selection of items performed in this way is called a _slice_, and is useful for exploring large data sets or forming new arrays from subsets of other arrays.\n",
    "\n",
    "For this data set, each example consists of a vector of _four_ features, and a _class label_ (five items total). Each of the four feature values are _continuous_ and the class labels are _discrete_. We can explore the data a little using the `unique()` function from numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the last column only...\n",
    "# .. and find the discrete set of items\n",
    "# that it contains...\n",
    "np.unique(data[:,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.3 2.  1.  0.1]\n",
      "[7.9 4.4 6.9 2.5]\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the other columns, too.\n",
    "# First, the minimum and next the maximum...\n",
    "print(np.min(data[:,0:4],axis=0))\n",
    "print(np.max(data[:,0:4],axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `unique()` function allows you to see the range of discrete values in an array. The 5th column of the data set contains only 0, 1, or 2. These are the _class labels_ for the examples. From a practical standpoint, these are the three different species of iris that we are wanting to classify. If you are interested in **exactly** which species the 0, 1, and 2 represent, please take a look at the details about the data set on the UCI repository page - [Link](http://archive.ics.uci.edu/ml/datasets/Iris). Understanding what the class labels correspond to in the real world might be important for understanding what our network is trying to tell us, but a proper vector encoding of the class labels alone is sufficient for training a neural network.\n",
    "\n",
    "The `min()` and `max()` functions allow us to explore the ranges of the four different measurements obtained from each flower. Each represents a measurement in centimeters for: sepal length, sepal width, petal length, and petal width. Again, this mapping becomes abstracted away since the neural network experiences each flower as a vector of measurements.\n",
    "\n",
    "At this time for simplicity, we will stick to the length-4 vector encoding of each flower. That is, the **training data** for the network that we will be building will just be all 150 vectors, each of length 4. We will store these **input vectors** in the matrix, $\\boldsymbol{X}$:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input training vectors\n",
    "X = data[:,0:4]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\boldsymbol{X}$ now contains all of the features for the 150 flowers we are going to try to classify using our neural network.\n",
    "\n",
    "For the class labels, we will construct a set of **target vectors** that represent the human-labeled class assignments for the flowers. While we _could_ just let the network attempt to assign a 0, 1, or 2, there is a more useful approach to take. This approach motivated by the idea of what kind of output a network should produce for a classification problem. In our case, if we left the class label vector as-is, the *target* vector for each flower would be a 1-dimensional vector (i.e. a single scalar value). Since we construct our network input and output _layers_ to have the same number of units as our input and target vectors, respectively, we would only use a single output unit. This _output unit_ would need to have an activation function capable of representing the possible target values (0,1,2) which limits our choices. However, with a different **encoding** of the target vectors, this will no longer be an issue, and we will get some other benefits along the way.\n",
    "\n",
    "Let's look at this method for transforming the class labels in detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First, let's grab the class labels by themselves...\n",
    "labels = data[:,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the labels \"in-hand\" we will use some tools from the `keras` package (which relies on the `tensorflow` package) to create the encoding we would like to use. We will first load the tools, then create the set of target vectors, $\\boldsymbol{Y}$, which we will be using for training our classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras/Tensorflow\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 3)\n"
     ]
    }
   ],
   "source": [
    "# Convert the integer class labels to a\n",
    "# categorical or \"one-hot\" encoding...\n",
    "Y = keras.utils.to_categorical(labels,\n",
    "                               len(np.unique(labels)))\n",
    "\n",
    "# Encoded vector size?\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this operation, the integer class labels have been encoded into a 3-dimensional vector space. The `unique()` function was used to determine the number of unique integer labels in the vector, and the vector itself gets passed to the `to_categorical()` function to produce this encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "[1. 0. 0.]\n",
      "1.0\n",
      "[0. 1. 0.]\n",
      "2.0\n",
      "[0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Some particular examples...\n",
    "print(labels[4])\n",
    "print(Y[4,:])\n",
    "\n",
    "print(labels[8])\n",
    "print(Y[8,:])\n",
    "\n",
    "print(labels[1])\n",
    "print(Y[1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, I have pulled three examples to illustrate how the encoding scheme works. The `0` class label has now been mapped to the vector $[1,0,0]$, the label `1` has now been mapped to the vector $[0,1,0]$, and the label `2` has been mapped to the vector $[0,0,1]$. This is known as a categorical (or _one-hot_) encoding, and is a common way to represent discrete (i.e. integer) information to a neural network. In fact, it can sometimes be useful to perform a similary mapping for integer data that is provided as _input_ to a network as well, but we will save that for another day.\n",
    "\n",
    "More importantly, since we now have vectors consisting only of scalar values in the range $[0,1]$, all typical activation functions that we have studied so far could be used since they can all produce output values in that same range (well, technically the acceptable range would be $(0,1)$ for the sigmoid function, for example, but that's often close enough in practice).\n",
    "\n",
    "Now that we have a set of **input patterns**, $\\boldsymbol{X}$, and a set of **target patterns**, $\\boldsymbol{Y}$, we can use these vectors for training a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Single-Layer Network\n",
    "\n",
    "Now that we have data ready for training, we just need to construct a network to learn how to to classify irises. The `keras` package provides the tools needed to set up such networks very quickly and start training them.\n",
    "\n",
    "We will start by setting up a data structure that will contain our network, known as the _model_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting up a single-layer network\n",
    "model = keras.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the `Sequential()` model which makes the assumtion that we would like to build a _feed-forward_ neural network architecture, which is what we have been focusing on so far in class.\n",
    "\n",
    "Now that we have the container, let's create a single layer network. We do this by adding it to the model using the `add()` member function. However, we also need to specify the kind of layer we want to add, and _some_ of its details. For our purposes, we are interested in adding a single layer (really, the _output layer_). Remember, the input layer is rather simple in that it doesn't perform computation, and instead just holds input pattern data during training and prediction (passing data through the network). So, we just need to tell this _output layer_ that it will receive data from the input layer of a certain size. We also need to create all of the connection weights between the input and output layer units, but this is all taken care of for us by the `Dense()` function.\n",
    "\n",
    "In a nutshell, when making a layer using the `Sequential()` model, all weights leading _into_ that layer will also need to be specified. There are different ways of connecting layers together, but for now we will mainly focus on densely connected networks, where all units in the previous layer will be connected to all units in the layer we are creating. Again, the `Dense()` function provides all of the functionality that we need for this operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a densely connected layer of units\n",
    "# and specify the input layer size (note,\n",
    "# the input layer is assumed to be there,\n",
    "# which makes this a single-layer network!)\n",
    "\n",
    "# Input size - 4\n",
    "input_size = X.shape[1]\n",
    "\n",
    "# Output size - 3\n",
    "output_size = Y.shape[1]\n",
    "\n",
    "# We are using a sigmoid activation\n",
    "# function, AND the input_size was\n",
    "# provided within a python list []...\n",
    "model.add(keras.layers.Dense(output_size,\n",
    "                             activation='sigmoid',\n",
    "                             input_shape=[input_size]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lovely new neural network!\n",
    "\n",
    "You can use the `summary()` function to get glimpse into what the Keras tools have created for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 3)                 15        \n",
      "=================================================================\n",
      "Total params: 15\n",
      "Trainable params: 15\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network follows the conventions we have utilized in class for neural units. That is the neural units, weight matrices, and bias weights have all been created for us using the `keras` tools.\n",
    "\n",
    "For example, we have a 4x3 weight matrix (12 connection weights), and three output units each with a bias weight, $w_{o}$, (3 bias weights total). Hence, we have 15 total weights that can be changed during the learning process, and these are known as _trainable parameters_ in the `keras` framework.\n",
    "\n",
    "The output units utilize the weighted sum calculation we have discussed in class (net input) and we have also specified a _sigmoid_ activation function for output.\n",
    "\n",
    "You can find some additional details about the network's structure by generating and image to display in your notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbgAAAC4CAIAAAAe325QAAAABmJLR0QA/wD/AP+gvaeTAAAgAElEQVR4nO3daVRT1/ow8H2ABEIgYVAMBFqsLUUcooILaaUoWNCFolAQB7ztteFShwWoOGDVVStotdgL94pDcWm1VEVYC1twqBYcLgotdEksWgbBCZKgiGFSwMh5P+y3538aNLMkgef3qWefzd7PoYfHMz6HIEkSAQAAeDUzQwcAAADGDhIlAACoAIkSAABUgEQJAAAqWNAXSktLv/nmG0OFAgAARmL16tV+fn7U4t+OKB88eJCXlzfgIQFgeGVlZWVlZYaOwojk5eU1NjYaOgrDyMvLe/DgAb3Fon+n3NzcgYoHAGMRFRWFYOenIQhi1apV8+fPN3QgBkAQhEILXKMEAAAVIFECAIAKkCgBAEAFSJQAAKACJEoAgJays7OJv9jY2CisvXfvXlhYWHt7O0JIIpGkpqb6+PhwOBwejxcQEJCfn6/L1GFhYQRBpKSk0Bs3bNiQk5Oj0HPDhg1UkFOmTNFuOkiUAOiks7PznXfemT17tqEDMZh9+/aRJNnZ2UlvrKys9PHxCQ4O5nA4CKHY2Nj09PStW7dKJJKysjJXV9eIiIgNGzZoN+PRo0cLCgr6t8fGxiYnJ2/evJne+NVXX5EkSZKkubm5dtMhSJQA6Igkyb6+vr6+PkMFYGNjM3XqVEPN/lLt7e1z5sz56KOPVq5cSTWmpaWFhoay2Wx3d/fDhw/z+fy0tLSHDx9qOrhYLE5MTFyyZEn/VaNGjcrPz09NTT158qROG9APJEoAdGJra1tfX3/mzBlDB2JEdu3aJZVKt2zZQrUUFhZ+/PHH1CKTyfTy8nrx4kVNTY2mg8fGxkZFRQUHB790rUAgiIyMXLNmjVwu1yLyV4FECQDQJ5IkDx486Ovr6+LioqRbQ0MDQsjBwUGjwQ8dOnTz5s20tDQlfcLDwxsbG0+fPq3RyMpBogRAe6dOnaJuFHR3dyu03L17Nzo62s7OztHRcfbs2fX19fin0tLScAdXV9fy8vKgoCBbW1tra+vp06dfvXoV90lJScF9qNPqc+fO4ZZhw4bRx+nq6rp69SpeZWHxknftBphIJGpubhYIBEr6HDlypL6+3sPDw8vLS/2RGxsb16xZc+jQIVtbWyXdJkyYgBD6+eef1R9ZJUiUAGhv3rx5JEnOnTv3pS2JiYmJiYlNTU05OTnFxcULFy7EfZKSkkiSFAgEMpksISEhJSVFKpVeuXKltbU1MDDw8uXLCKFNmzaRJMlms6mRZ86cSZKkt7c31YLHYbPZ77//Pr5fQT/fDAwMdHR0HPgX2KuqqhBCrq6uL11769athISEpUuX2tvb45vm6o8sFAoXLVoUGBiovBufz6fC0BdIlAC8LkKh0M/Pj81mz5gxIzQ0tLy8vKWlhd6hq6tr7969uI+Pj092dnZvb29CQoJeZu/r68PZUy+jqU8ikSCEuFzuS9eOHz8+Nzd35cqVVVVVkydPVn/YrKysurq6Xbt2qezJ4XAIgsBh6IvhD9QBGKzoicDNzQ0hJBaLqRNnhBCbzcbnidi4ceNcXFxEIpFEInF2dtZx9kuXLuk4gnbwJQgGg/GqDsXFxZ6enhqNef/+/bVr1/7444/0Q2wlLCwsnj17ptEUysERJQCvC/2oislkIoQUniKys7NT+BEnJyeEkBYPzRgPKysrhNDz58/1OGZBQUFbW9u0adOo67/48aDNmzfjxdu3b9P7y+VyFoulxwAgUQJgMI8fP1Y4NcYpEqdLhJCZmVlvby+9g0wmUxhEo8t8AwAfC7e1telxzBUrVpB/9/333yOEtm3bhhfffvttqnN7eztJkrofktNBogTAYLq7u8vLy6nFP/74QywWCwQC6o/c2dm5qamJ6iCVSu/fv68wiLW1NZVM33333W+//fY1R63C2LFjEUKvKvorl8s1Pe/WFP6N4TD0BRIlAAbD5XI3btxYWlra1dVVUVERExPDZDIzMjKoDsHBwWKxeM+ePZ2dnfX19QkJCdTBJmXSpEm1tbUPHjwoLS1taGjw9/fH7Ya66y0QCJycnEQiUf9VWVlZbDZ73bp1/VfFxMQQBHHnzh3dA6isrEQIveqJdO1AogRAe/ipyR9//BEhxGKxYmJiysrK6C2bNm1CCBEEsXPnToTQxIkT6W+F29jY/Pe//926dauzs/MHH3xgb29fXFwcEBBAdUhJSREKhdu3b3dycvrkk0/Wrl3L4/EeP35MEAT1onR6evr48eNHjx4dHR2dkZExevRo3C6Xyw1y15sgCKFQ+Ouvv4rFYoVVSm7ESyQSGxubN954Q+X4n332mcI1ypkzZ9I75Ofn8/n80NBQHTaiH/ppPy68QQIw9ERGRkZGRg7kjAKBgM/nD+SMGkEI5eTkKO+DLxTiohh0MpmMz+fHxcWpOdeTJ09YLJZQKNQm0L+rrKwkCOL48eP9V5mbm/v6+qozSP9thyNKAICecbncgoKCvLy8zMxMlZ1JkoyPj+dwONu2bdNx3oaGhoiIiOTk5AULFug4lAI9JMoTJ07gO/T4sQDwWtnY2BA0yl96HUhGGxh43ZYtW9a/HuXEiRMrKirOnj2L61Eq0dzc3NDQUFRUxOPxdIzkwIEDqampqamp9EaqHuWLFy+0H5p+eKnLqXdQUJClpaV2PzvwOjo63n777dDQUEMHoo3r168jhObOnWvoQBQZbWDqGMhT76+//pr+N/j5558PzLwaQWqceg9W/bd9iJ56k1BDUAcmHbwxwO9oUxTKdAMjNERfYcQ1BA0dBQDANAzRI0oAAFCflomyurp63rx5XC6XzWb7+/uXlJT07/Po0aP4+Hh3d3cmkzl8+PCIiAj8IChSr2YfQqinp2fLli2enp7W1tYODg5z5sz56aef6FdklUyhxCCrIWhCwcvl8pycnA8//JDH47FYrHHjxmVkZOALIDKZjH4vCJ+NyuVyqiUyMhIPouZ+VVNTM3/+fEdHR7yoULYHAM3Qr5WoeTOnrq7Ozs6Oz+efP3++o6Pjxo0bwcHB7u7u9Js5YrH4zTffHDFixOnTpzs6OqqqqgICAqysrK5du0b1wTX75s6de+3atc7OzgsXLrBYrMmTJ1MdhEIhl8s9f/7806dPpVJpUlISQujixYvqT6EEnv3Zs2fqx0OSpEAgYLPZfn5+uE95efn48eOZTOalS5eoPvT6gJi3t7ejoyO9pX8fbPr06Q4ODqWlpUoif+k9E2MIXuXNHPxBqO3bt7e2tj569Og///mPmZkZ/YJdSEiImZnZ7du36T/l5+f3ww8/4P9Wf78KCAi4ePFiV1dXWVmZubn5o0ePXhUVNvDPURo5BDdz6C30BTUTZVRUFEIoLy+PamlqarK0tKQnSvx9DGr/JklSIpFYWlp6e3tTLXiHLigooFrwUQO1T48cOfK9996jT+3h4UElSnWmUOJViVJJPCRJ4rrN169fp1pu3LiBEBIIBFSLLrkmICDA3t5eea5XkigNG7w6iXLatGn0lpiYGAaD0dbWhhdxVerly5dTHUpKSvh8fm9vL15Uf786c+bMq8J4KUiUCiBR/q2FvqBmosR12Ds6OuiN48aNoydKLpdrZmZG/QFgkyZNQgg9ePAAL+IdWiqVUh1WrVqFEBKJRHhx2bJlCKHY2NjS0lLqfSyNplDiVYlSSTzkXwdlCkPhb4OIxWK8qEuuUYeSRGnY4LV4PAg/KEP/h2HcuHHW1tYtLS3UdlGfGyU12a+oEdREndoDgPolSo2vjvX09HR0dFhZWSk8X+rk5FRbW0v1wUWWXlrluK6ujl4mXknNvszMTD8/vyNHjgQFBSGE/P394+LiwsPDNZ1CI9rVEBSLxQ8fPtRvZSctGHnwbW1tu3fvzs/Pb2xspJcLe/r0KfXfiYmJn3766d69ezdv3lxbW1tcXHz48GG8SqP/6WpWeKWbMmUK/tcFIISio6MTExP9/PwMHYgBREdHK7RonCgtLS1tbW07Ojo6OzvpubK1tZXex87OrrOz89mzZ7rcqcCvvi9ZsuT58+eXLl1KS0uLiIjYvXv36tWr9TWFFnANQYJWBNCEaggaNvg5c+b873//y8jIWLhw4bBhwwiCSE9PX7VqFUmrkrB48eKNGzfu2bNn3bp1u3fv/vjjj+3t7fGq1/0/3dXVdf78+Xof1kRFR0f7+fkNzV9I/0SpzV3vWbNmIYTOnTtHtbS0tCh8nzciIkIul1N3VLGdO3e+8cYb6n9v187Orrq6GiHEYDA+/PBDfE+T+gqlXqbQgknXEDRU8BYWFjdv3rx69SqPx4uPjx8+fDjOtv3r9VtaWi5fvvzhw4e7d+/+4YcfFD4gY6j/6WCI0yZRbt++3cHBITEx8cKFC52dnbdu3YqJiVE4E9+xY8eoUaOWLl169uzZtra21tbWAwcOfPnll2lpaRodC3z22Wc3btzo6el5+PDhrl27SJKkvsGmryk0ZdI1BF9r8MqZm5tPmzZNKpV+/fXXLS0tz549u3jx4v79+/v3XL58OS5QNmPGDHrlamS4/+lgqKNfsFT/Xe+ampp58+ZxOBz8DEphYSG+jIgQ+vTTT3Gfx48fr169+q233mIwGMOHDw8ODr5w4QJeVVpaSo8Bv+tKb8FvYVdWVsbFxY0ePRo/RzllypSsrCyqpJ3yKZTIz8+nz7V48WI14yH/Ko1169atkJAQW1tbFosVEBBQUlJCH18mkwmFQmdnZxaLNXXq1PLycuoTo+vXr8d9qqur/f392Wy2m5tbZmYm9bP+/v7K73orXHr7+uuvjSR4ldcE//zzz0ePHsXFxbm5uTEYjBEjRnzyySdUUUWFZxViY2MRQpcvX+7/G1B/v0KaFC6Au94KENz1piFI2h/VyZMno6OjyQGv9GlCJkyY0NLS8qoy90bOhII/fPhwZmZmRUXFgM2IH3rLzc0dsBmNHEEQOTk5Q/MaZf9th1cYgTHav3//6tWrDR0FUCE7O5t6FUrh4htC6N69e2FhYbjMmkQiSU1N9fHx4XA4PB4vICBA4cROU2FhYdQbXJQNGzbg02KFRirIKVOmaDcdJEpgLA4ePBgeHt7Z2bl///4nT54MzWMZU4QrnHd2dtIbKysrfXx8goODORwOQig2NjY9PX3r1q0SiaSsrMzV1TUiIoK68KKpo0eP4re8FMTGxiYnJ2/evJneSD2Ha25urt10aBAnSuLVvvjiCy0GxK85i0SipqYmgiDwt1BMhakEf+rUKXt7+3379p04cWJw35x53aXqDFsKr729fc6cOR999NHKlSupxrS0tNDQUDab7e7ufvjwYT6fn5aWpsUXzMVicWJiIv5mjoJRo0bl5+enpqaePHlSpw3oZ9Dui3q/0pqUlIRfNjdFJhG8UCgUCoWGjgLowa5du6RS6ZYtW6iWwsJCegcmk+nl5dXU1FRTU9P/sQrlYmNjo6Ki/P398Rd7FAgEgsjIyDVr1kREROjx39pBe0QJADAIkiQPHjzo6+uL3459lYaGBoSQg4ODRoMfOnTo5s2byj80Eh4e3tjYSD1wrReQKAHQDH4+adSoUUwm097eftasWRcvXsSrdClVZySl8HQnEomam5txBZZXOXLkSH19vYeHh5eXl/ojNzY2rlmz5tChQ7jcxKtMmDABIYQLrOgLJEoANCCVSidPnnzs2LGMjIyWlpZff/3V2to6KCjo4MGDCKFNmzaRf3+kdObMmSRJUs+ior++A0EvLILfKcLtAoFAJpMlJCSkpKRIpdIrV660trYGBgZevnxZx/Gx1/1GA0KoqqoKIfSqYgu3bt1KSEhYunSpvb09vmmu/shCoXDRokXUKyevwufzqTD0BRIlABpITk6+c+dOenr67NmzORyOh4fHsWPHnJ2d4+Pjm5ub9TJFV1fX3r17/fz82Gy2j49PdnZ2b2+vwqucWqPe19DLaC8lkUjQKwqXIITGjx+fm5u7cuXKqqqqyZMnqz9sVlZWXV3drl27VPbkcDgEQeAw9AUSJQAawE//hYaGUi2WlpZBQUHPnj3T17kem83GJ4/YuHHjXFxcRCKRXv7yL1261Nra+lprAuGvBjAYjFd1KC4uzsjIUH4FU8H9+/fXrl176NAhNYtCWVhY9C8joAtIlACoC9d5s7KyUrhGNmLECISQVCrVyywvLYWH/qrzZPysrKwQQs+fP9fjmAUFBW1tbdOmTaMe8sOPB23evBkv3r59m95fLpezWCw9BgCJEgB1WVpacrnc7u7ujo4Oejs+6ebxeHhRx1J1uBQevcWE6vghhHAlKlw5VF9WrFih8Do2fjZo27ZteJFePKW9vZ0kSf3WV4VECYAGcN1o+qMnPT09RUVFLBYrJCQEt+hYqs6k6/ghhMaOHYsQelVJAblc7unp+VoDwL8cHIa+QKIEQAM7duwYOXJkYmJiYWFhR0dHbW3tokWLJBJJRkYGPgFHOpeqM+k6fgghgUDg5OQkEon6r8rKymKz2evWreu/KiYmhiCIO3fu6B4A/ipncHCw7kP9H/rRrPpl1gAYZNQvs9bS0pKYmDhy5EgGg8HlckNCQoqKiugddKmzZ/A6fhSkRpk1fP6L3/Wm27hxo4WFRVNTk0L7/v37WSwW/bublMDAQBsbm/6fxuovLi5OIYmFhITQO0RFRdE/SEcxNzf39fVVOT6pr4+LATD4GEk9SpwoDR0FSeqWKGUyGZ/Pj4uLU3OuJ0+esFgsoVCoTaB/V1lZSRDE8ePH+6/SJVHCqTcAQM+4XG5BQUFeXl5mZqbKziRJxsfHczicbdu26ThvQ0NDREREcnLyggULdBxKASRKAIBOli1b1r8e5cSJEysqKs6ePYvrUSrR3Nzc0NBQVFREPTagtQMHDqSmpqamptIbqXqUL1680HpkSJQAGAVTKYVHFxMTQ52cKtSjRAi5u7sXFhbiepRK8Hi8kpKSMWPG6B7Pzp07+x9L0r8Lr/VdrEFbZg0A02ISpfCGLDiiBAAAFSBRAgCACpAoAQBABUiUAACgwktu5uj9uzwAGD/8bjLs/HSlpaWGDsFo0J8+7/9JXAAAGIIU3swhyNdZ6xgATeHPecORHTAqcI0SAABUgEQJAAAqQKIEAAAVIFECAIAKkCgBAEAFSJQAAKACJEoAAFABEiUAAKgAiRIAAFSARAkAACpAogQAABUgUQIAgAqQKAEAQAVIlAAAoAIkSgAAUAESJQAAqACJEgAAVIBECQAAKkCiBAAAFSBRAgCACpAoAQBABUiUAACgAiRKAABQARIlAACoAIkSAABUgEQJAAAqQKIEAAAVIFECAIAKkCgBAEAFSJQAAKACJEoAAFABEiUAAKgAiRIAAFSwMHQAYKi7cuVKaWkptVhdXY0Q2rlzJ9Xi5+f3wQcfGCAyAP5CkCRp6BjAkFZUVDRjxgwGg2Fmpnh+09fX9/z5819++SUoKMggsQGAQaIEBtbX18fj8R49evTStcOGDZNKpebm5gMcFQB0cI0SGJiZmdnixYuZTGb/VUwmMyYmBrIkMDhIlMDwFi5c2Nvb27+9t7d34cKFAx8PAArg1BsYBXd393v37ik0urm53bt3jyAIg4QEAAWOKIFRWLJkCYPBoLcwGIxPPvkEsiQwBnBECYxCdXX16NGjFRqrqqrGjBljkHgAoIMjSmAUPD09x4wZQz9+9PLygiwJjAQkSmAs/vGPf1A3uBkMxscff2zYeACgwKk3MBYPHjx488038Q5JEERDQ4O7u7uhgwIAITiiBMbDzc3N19fXzMzMzMzM19cXsiQwHpAogRFZsmQJQRBmZmZLliwxdCwA/B849QZGpKWlhcfjIYTEYrGTk5OhwwHg/zP5RAnP2QFg/Ew9zwyGMmuJiYl+fn6GjgJo49///jdCaNWqVVTLlStXCILw9/c3XFCGFB0dPcj259LS0vT0dENHoavBcESZk5Mzf/58QwcCtBEVFYUQys3NpVo6OjoQQra2tgaLyaAG3/588uTJ6OhoU88zg+GIEgwmQzZFAmMGd70BAEAFSJQAAKACJEoAAFABEiUApurevXthYWHt7e0IIYlEkpqa6uPjw+FweDxeQEBAfn6+LoOHhYURBJGSkkJv3LBhQ05Ojk5BmyZIlMAkdXZ2vvPOO7NnzzZ0IAZTWVnp4+MTHBzM4XAQQrGxsenp6Vu3bpVIJGVlZa6urhERERs2bNBu8KNHjxYUFPRvj42NTU5O3rx5s06hmyBIlMAkkSTZ19fX19dnqABsbGymTp1qqNnb29vnzJnz0UcfrVy5kmpMS0sLDQ1ls9nu7u6HDx/m8/lpaWkPHz7UdHCxWJyYmPjSt0hHjRqVn5+fmpp68uRJnTbA1ECiBCbJ1ta2vr7+zJkzhg7EMHbt2iWVSrds2UK1FBYW0gvTMZlMLy+vFy9e1NTUaDp4bGxsVFRUcHDwS9cKBILIyMg1a9bI5XItIjdRkCgBMDEkSR48eNDX19fFxUVJt4aGBoSQg4ODRoMfOnTo5s2baWlpSvqEh4c3NjaePn1ao5FNGiRKYHpOnTpF/KW7u1uh5e7du9HR0XZ2do6OjrNnz66vr8c/lZaWhju4urqWl5cHBQXZ2tpaW1tPnz796tWruE9KSgruQ51Wnzt3DrcMGzaMPk5XV9fVq1fxKguLAX1xQyQSNTc3CwQCJX2OHDlSX1/v4eHh5eWl/siNjY1r1qw5dOiQ8sf+J0yYgBD6+eef1R/Z1EGiBKZn3rx5JEnOnTv3pS2JiYmJiYlNTU05OTnFxcXUB2+TkpJIkhQIBDKZLCEhISUlRSqVXrlypbW1NTAw8PLlywihTZs2kSTJZrOpkWfOnEmSpLe3N9WCx2Gz2e+//z5JkiRJ0k9CAwMDHR0dy8rKXt/mV1VVIYRcXV1fuvbWrVsJCQlLly61t7fPzs7WqGqMUChctGhRYGCg8m58Pp8KY4iARAkGG6FQ6Ofnx2azZ8yYERoaWl5e3tLSQu/Q1dW1d+9e3MfHxyc7O7u3tzchIUEvs/f19eHsqZfRXkoikSCEuFzuS9eOHz8+Nzd35cqVVVVVkydPVn/YrKysurq6Xbt2qezJ4XAIgsBhDBHwrjcYbOjZwc3NDSEkFoupE2eEEJvNxieP2Lhx41xcXEQikUQicXZ21nH2S5cu6TiCSvhqg8LXfemKi4s9PT01GvP+/ftr16798ccf6UfTSlhYWDx79kyjKUwaHFGCwYZ+qMVkMhFCCk8R2dnZKfwILhKsxZM0BmFlZYUQev78uR7HLCgoaGtrmzZtGnWpFz8etHnzZrx4+/Zten+5XM5isfQYgJGDRAmGnMePHyucGuMUSdVUNzMz6+3tpXeQyWQKgxiwYjQ+7G1ra9PjmCtWrCD/7vvvv0cIbdu2DS++/fbbVOf29naSJHU/+jYhkCjBkNPd3V1eXk4t/vHHH2KxWCAQUH/5zs7OTU1NVAepVHr//n2FQaytralk+u6773777bevOer/M3bsWIRQY2PjS9fK5XJNz7s1hX85OIwhAhIlGHK4XO7GjRtLS0u7uroqKipiYmKYTGZGRgbVITg4WCwW79mzp7Ozs76+PiEhof8HfCZNmlRbW/vgwYPS0tKGhgaqJPsA3PUWCAROTk4ikaj/qqysLDabvW7duv6rYmJiCIK4c+eO7gFUVlYihF71RPrgRJo4hFBOTo6howBaioyMjIyM1PSnFMo9LF68uLS0lN7y+eefk38/uQ4NDcU/KxAI+Hz+rVu3QkJCbG1tWSxWQEBASUkJfXyZTCYUCp2dnVks1tSpU8vLy6nHg9avX4/7VFdX+/v7s9lsNze3zMxM6mf9/f3t7e2vXbum3S9Ezf1548aNFhYWTU1NCu379+9nsVj4ASYFgYGBNjY2crlc5eBxcXEKWSIkJITeISoqis/n9/b2qhyKJElcREOdnsbM9DcAEqUp0y5R6gInyoGcUSNq7s8ymYzP58fFxak57JMnT1gsllAo1C06kiTJyspKgiCOHz+uZv/BkSiH4qn3iRMn8I08fPfQtNjY2BA0ZmZm9vb2AoFg+fLlv//+u6GjAwOEy+UWFBTk5eVlZmaq7EySZHx8PIfD2bZtm47zNjQ0REREJCcnL1iwQMehTMtQTJQLFiwgSTIoKMjQgWijs7Pz+vXrCKG5c+eSJPn8+fPq6uovv/yyurrax8fnn//859OnTw0dIxgIEydOrKioOHv2LK5HqURzc3NDQ0NRURH+ZrouDhw4kJqampqaquM4JmcoJsrBxNzcfMSIEXPnzi0uLl63bt133323cOFC0sS/ePea4He0RSJRU1MTQRCbNm0ydES6cnd3LywsxPUoleDxeCUlJWPGjNF9xp07dw61Y0kMEuXg8dVXX/n6+v70008nTpwwdCzGSOEWh0LtbgCUgEQ5eBAEgcu47t2719CxADCoDJVEWV1dPW/ePC6Xy2az/f39S0pK+vd59OhRfHy8u7s7k8kcPnx4REQEfl4MqVfFCyHU09OzZcsWT09Pa2trBweHOXPm/PTTTy9evFBnCr3AxcHKysqo99sGwUYBYHiGuNWuT0iNxynq6urs7Oz4fP758+c7Ojpu3LgRHBzs7u5uaWlJ9RGLxW+++eaIESNOnz7d0dFRVVUVEBBgZWVFfyAOV/GaO3futWvXOjs7L1y4wGKxJk+eTHUQCoVcLvf8+fNPnz6VSqVJSUkIoYsXL6o/xfTp0x0cHEpLS5VsDv1mjgKqToFYLDaejVJi4B8PMnLq7M+mZXA8HmT6G6DGjhUVFYUQysvLo1qamposLS3piRKX0f/hhx+oFolEYmlp6e3tTbXgnFJQUEC1REZGIoQePXqEF0eOHPnee+/Rp/bw8KByijpTBAQEqHxcWUmipG5540RpJBulBCRKBZAojdOQKLN27tw5hFBISAjV4uLi4uHhUVtbS7WcOnXKzMyM/lU/Ho83ZsyY33//vbGxkV4kVUkVr5kzZ+7bt+9f//rX0qVLJ0+ebG5uTv9iiahA+RoAAAnCSURBVDpT6FikC5cIZDAYOB4j2SjlGhsbh9qXqpRTeMvI1A2OzRn8ibKnp6ejo8PKysrGxobe7uTkRCXKnp4eXIvlpcVQ6+rq6H/wSqp4ZWZm+vn5HTlyBD+k6e/vHxcXFx4erukUWsPXXv38/BgMhqlsVFlZWXR0tAYbOdilp6enp6cbOgrwN4P/Zo6lpaWtrW13d3dnZye9vbW1ld7Hzs7OwsLi+fPn/Y+6p0+fruZcuIrfL7/8IpPJTp06RZJkRETEN998o8cplOjr68PvaaxYscKENgpOvenQID31NnWDP1EihGbNmoX+OgHHWlpaFD7jGRERIZfLqY9MYTt37nzjjTfU/yynnZ1ddXU1QojBYHz44Yf4tjL1sTq9TKFEcnLyb7/9Fh4ejq/J6mtGw24UAEbB0P/e6Aqp8S/w7du3HRwcqLveN2/eDAkJcXJyot/MaW5uHjVq1FtvvXXmzBmZTPb48eP9+/dbW1vTB8f3PZ49e0a1rF+/HiF0/fp1vMjlcgMCAkQiUXd3d3Nz8xdffIEQSklJUX8KTe96v3jxorm5+dSpU/iDUEuXLn369KmxbZQScDNHgTr7s2kZHDdzTH8D1Nuxampq5s2bx+Fw8LMvhYWF1Lven376Ke7z+PHj1atXv/XWWwwGY/jw4cHBwRcuXMCr1KziVVlZGRcXN3r0aPzI4ZQpU7KysqivTSmfAlNZpEvhkyYEQXC53HHjxi1btuz333/v398YNkoJSJQKIFEaJ4I08feCCYLIycmZP3++oQMB2sBXCXJzcw0diLEYfPvzyZMno6OjTT3PDIlrlAAAoAtIlACYqnv37oWFheEyaxKJJDU11cfHh8Ph8Hi8gIAAhTrwKpEkefXq1RUrVnh4eFhaWjo5OU2dOjU7O5t+MLhhw4bBcRdbU5AoATBJlZWVPj4+wcHBuMxabGxsenr61q1bJRJJWVmZq6trRETEhg0b1B+wpqZm6tSptbW1eXl5bW1tZWVlb7zxxpIlS9auXUv1iY2NTU5O3rx5s/63x8gZ9hKp7tCgu/g9pAzwzRw2m/3+++8b8/hq7s9tbW2urq70T0GEhoZ+99131GJPTw+fzzc3N29ublZz6j///NPCwqK1tZU+iKOjo6WlZXd3N9WIPwWh/h/d4LiZA0eUAJieXbt2SaXSLVu2UC2FhYX4vXuMyWR6eXm9ePFC4XlhJTw9PZ8/f25vb08fxM3Nraenp7u7m2oUCASRkZFr1qwZUs/JQqIEwMSQJHnw4EFfX18XFxcl3RoaGhBCDg4OWk8kk8nq6uomTpyo8IpqeHh4Y2Mj9dLBUACJEpgG/LTmqFGjmEymvb39rFmzLl68iFelpKTgqpq4HCdC6Ny5c7gF1/VAf30Hoqur6+rVq3iVhYUF1U4QhKura3l5eVBQkK2trbW19fTp06nXjXQZ/3UQiUTNzc0CgUBJnyNHjtTX13t4eHh5eWkxRXt7+9WrV8PCwng83tGjRxXWTpgwASH0888/azGyqTL0ub+uEFyjNGVqXqOUSCQjR44cMWJEQUFBW1tbTU1NREQEQRBZWVlUn/7XB729vR0dHektr7qGKBAI2Gy2n58fLspZXl4+fvx4JpN56dIlvYyvzgtXmDr78/fff48Q2r59+0vX3rx5Mz4+Hn+e87ffflM5Y3/UxxqnTZt248aN/h1wJRR/f391RoNrlAAMkOTk5Dt37qSnp8+ePZvD4Xh4eBw7dszZ2Tk+Pr65uVkvU3R1de3du9fPz4/NZvv4+GRnZ/f29iYkJOhlcOpFJr2MhovpvbRiE0Jo/Pjxubm5K1eurKqqolfPU9+mTZt6enr+/PNPT0/PiRMn9v/ILYfDIQgChzFEQKIEJgA/EhgaGkq1WFpaBgUFPXv2TF8ngGw2G59RYuPGjXNxcRGJRHpJB5cuXWptbfXz89N9KIQQvrXCYDBe1aG4uDgjI0P5FUzlmEymp6fnvn37wsLCtmzZ8ssvvyh0sLCwoMrpDwWQKIGxw1UvraysbG1t6e0jRoxACEmlUr3MYmdnp9Di5OSEEHr48KFextcjKysrhBD1WaTXas6cOQihwsJChXa5XM5isQYgACMBiRIYO0tLSy6X293d3dHRQW/HJ908Hg8vmpmZ9fb20jvIZDKFoQiCeNUsjx8/Vjg1xikSp0vdx9cjZ2dnhBC+UPi6WVpaor8Xb0UItbe3kySJwxgiIFECE4ALqtOfR+np6SkqKmKxWNQXPpydnZuamqgOUqn0/v37CuNYW1tTye7dd9/99ttvqVXd3d3l5eXU4h9//CEWiwUCAZUOdBxfj8aOHYsQamxsfOlauVzu6empxbBJSUkxMTEKjWfPnkV//1IIQgj/HnAYQwQkSmACduzYMXLkyMTExMLCwo6Ojtra2kWLFkkkkoyMDHwCjhAKDg4Wi8V79uzp7Oysr69PSEigDgYpkyZNqq2tffDgQWlpaUNDg7+/P7WKy+Vu3LixtLS0q6uroqIiJiaGyWRmZGRQHXQZPzAw0NHRsaysTC+/DYFA4OTkJBKJ+q/Kyspis9nr1q3rvyomJoYgiDt37igZ+dixY19++eXdu3d7enru3r27fv367Oxsb29voVBI74Y/RxwcHKzbdpgUQ95y1wcEjweZMvVfYWxpaUlMTBw5ciSDweByuSEhIUVFRfQOMplMKBQ6OzuzWKypU6eWl5d7e3vjnXz9+vW4T3V1tb+/P5vNdnNzy8zMpH5WIBDw+fxbt26FhITY2tqyWKyAgICSkhJ9ja+yzChFzf1548aNFhYWTU1NCu379+9nsVhJSUn9fyQwMNDGxkYul79qzLa2toMHD4aEhOBPtNvY2Hh7e+/YsYNeChqLiori8/m9vb0q4yQHy+NBpr8BkChNmZEU7sWJ0tBRkKTa+7NMJuPz+fR3vZV78uQJi8USCoW6RUeSf73rffz4cTX7D45ECafeAJgeLpdbUFCQl5eHPyenHEmS8fHxHA6n/xORmmpoaIiIiEhOTl6wYIGOQ5kWSJQAmKSJEydWVFScPXsW16NUorm5uaGhoaioiHpCQGsHDhxITU1NTU3VcRyTA4kSDGn4HW2RSNTU1EQQxKZNmwwdkQbc3d0LCwtxPUoleDxeSUnJmDFjdJ9x586dQ+1YEntd7+0DYBKSkpKSkpIMHQUwdnBECQAAKkCiBAAAFSBRAgCACpAoAQBABYI08Q+TEwQxZcoUV1dXQwcCtIHf6psyZYqhAzEWeXl5g2x/bmxsLCsrM/k8Y+obEBUVZegQAAAq5ObmGjoEnZh8ogQAgNcNrlECAIAKkCgBAEAFSJQAAKACJEoAAFDh/wFAK10BqNquXAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.utils.plot_model(model,to_file='single_layer_network.png',\n",
    "                       show_shapes=True,expand_nested=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that some additional information (like details about the *input layer*) are provided by this format. You can also see how the layers are connected together (directional arrows), their layer types (eg. Dense), and their input/output shapes.\n",
    "\n",
    "There are two things to keep in mind at this point, and the diagram above illustrates these concepts:\n",
    "\n",
    "1. The input shape for the Dense layer is `(?,4)` and the output shape for the Dense layer is `(?,3)`. First of all, each of these shapes indicate that they each are a **2 dimensional** tensor (also known as a matrix). Second, the `?` is a flexible placeholder for the length of the first dimension of the tensor. The reason for this is because this network is built to accept patterns of length 4, but the network cannot know how many patterns it will be dealing with at any particular moment in time. Remember, we can utilize parallel processing in many ways in neural networks, and one way is to simply have the network process multiple inputs vectors/patterns at the same time (weights are held constant while this entire groups of vectors is propagated through the model). **If** we wanted to have it operate on a **single pattern**, we could provide it with an input tensor of shape `(1,4)` (one pattern on each row, only one row). However, we could pass *more patterns* into the network at the same time by just **adding additional rows of input patterns** to the tensor. The `?` for all layers is replaced by the number of patterns (the length of the first dimension in the tensor). So, the output from the network would correspondingly be a `(1,3)` tensor (one pattern produces an output activation vector of length 3 - remember, we have 3 iris species).\n",
    "\n",
    "2. The weights for the Dense layer are stored internally inside of the Dense layer itself, so the arrow is just showing how the output tensor from the Input layer will be copied/sent downstream to the Dense layer. The Dense layer's input shape is of the corresponding shape to receive it, which is a requirement for a properly structured network. However, the fact that the Dense layer has an input shape of `(?,4)`, and an output shape of `(?,3)` means it will contain a typical dense layer weight matrix of shape 4x3 and 3 bias weights (for computing net inputs, see above), and then perform a rate encoding using the chose activation function (sigmoid - but note that this is *not shown* in the diagram).\n",
    "\n",
    "However, the model isn't quite ready to go. At this point, `model` contains only a **template** for what we want the network to be. We need to `compile()` the network to create the tensorflow data structures that _actually_ compute the neural network. Let's do that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep the model for -learning-\n",
    "model.compile(loss=keras.losses.mse,\n",
    "              optimizer=keras.optimizers.SGD(lr=0.01),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, learning requires some method for specifying how to update the weights via experience with the training data.\n",
    "\n",
    "We will use the _stochastic gradient descent_ to perform this operation, and we select this by setting `optimizer = keras.optimizers.SGD(lr=0.01)` (note, the learning rate, `lr`, setting). However, we need to select an _error function_ as well that we would like to minimize for the optimizer to know what to optimize.\n",
    "\n",
    "In the current literature, since not all functions for describing goodness are really metrics for measuring _error_, the more general term, _loss_, is often used. We will be using a _loss_ function that is very similar to the SSE function we studied in class, but here it's the Mean-Squared-Error (`loss=keras.losses.mse`). Overall, you can think about this as being similar to multiplying the SSE by some fraction (like we did to derive the delta rule) based on the number of training examples used in each batch for calculating the weight update.\n",
    "\n",
    "While we will use loss to optimize the weights in the network, a more intuitive metric of performance is added to the model as well: _accuracy_. While accuracy isn't something used for optimization, if we assume that the strongest ouput from the network (whichever of the three output units produces the highest value) is the network's _best guess_ at what the current iris example should be, then we can calculate the fraction of the iris patterns that it is classifying correctly. Thus, 0.0 accuracy would indicate that the network is classifying **none** of the examples properly, but 1.0 accuracy would indicate that the network is classifying **all** of the examples properly.\n",
    "\n",
    "One final important note: this neural network uses a *sigmoid* activation function for it's output layer and MSE is known to **not** be the best choice of loss function for output layers of this type. MSE is more commonly used with a linear activation function which maps to continuous outputs (regression problems instead of classification problems). However, since MSE keeps the delta-rule calculations below very simple, we have opted to use it here. In future lectures, we will discuss how to use a more principled approach to pairing output layer activation functions with loss functions since this can greatly improve a network's learning behavior.\n",
    "\n",
    "There are other things that the `compile()` function takes care of for us, such as setting the weights to some reasonable starting values. For now, we will trust the `compile()` function to do this job. However, we can always catch a glimpse of what the current weight values are in the network if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.09391999, -0.40792686,  0.35855067],\n",
       "        [ 0.5989735 ,  0.703128  ,  0.5537844 ],\n",
       "        [-0.23786509,  0.6430185 ,  0.728176  ],\n",
       "        [ 0.6450845 ,  0.71855295,  0.31226158]], dtype=float32),\n",
       " array([0., 0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the bias and connection weights...\n",
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A python list is returned: the first element contains the 4x3 weight matrix, and the second element contains the 3-element vector of output unit bias weights. You can see that the connection weights are initialized to _small_ random values, and the bias weights are initially set to zero. We will explore other methods for initializing the weights in later assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does it work?\n",
    "\n",
    "Before we move on, let's quickly explore how `keras` calculates values for the network. In other words, let's perform the forward-pass process of the neural network from _scratch_ using `numpy`. This way, we can see that `keras` is actually calculating the expressions for single-layer networks just like how we explored them in class.\n",
    "\n",
    "First, keep in mind that the weights above are incorrect (they are random initial values), and will be changed during _training_ below. However, we can still see what the _current_ predictions of the network will be. That is, given some input patterns, $\\boldsymbol{X}$, what would the network output for those examples? The `predict()` function will allow `keras` to do this work, and we will choose to perform this operation for the first _five_ patterns in the network. After that, we will explore applying the forward-pass equations by computing them outselves using the same weights above to see how `keras` computes a forward-pass in just the same was as we have discussed in lecture.\n",
    "\n",
    "So, let's call the `predict()` function using the first five training patterns. In then end, this function will give us the final activations for the _output layer_ neurons. Since we have a one-hot encoding strategy with three values for our targets, $\\boldsymbol{Y}$, we also asked `keras` to build the output layer using an equivalent number of neurons. Therefore, if we provide 5 training patterns as input, we will expect the `predict()` function to return a 5x3 matrix where each row contains the output activations for one of the five patterns. It is easy to process more than one pattern using this function due to the nature of linear algebra operations as we will see below when we compute the __same__ activations from scratch using `numpy` to implement the forward-pass equations.\n",
    "\n",
    "One final note... since the weights are randomly initialized, we will get essentially useless outputs at this time since the network must first be trained (and the weights modified) to perform better at producing the desired function. However, once the network _has_ been fully trained, then these weights will produce an excellent predictions on the Iris task. Thus, `predict()` is more typically used _after_ training instead of _before_. Yet, all that the `predict()` function really does is perform a forward-pass through the network with the provided data, and the outputs are assumed to be the _current_ predictions of the network whether trained or untrained. Therefore, mimicking this process using `numpy` is a good way to see what the function is actually doing...\n",
    "\n",
    "OK, enough talk, let's get to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.88167536, 0.9479702 , 0.9988756 ],\n",
       "       [0.9292399 , 0.9872105 , 0.999846  ],\n",
       "       [0.92776287, 0.98947513, 0.9999311 ],\n",
       "       [0.89161384, 0.9660111 , 0.99920636],\n",
       "       [0.93063146, 0.85484225, 0.99390864]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the predicted outputs for these first five input patterns using the model we built above. Now we will utilize the weights directly via `numpy` to perform the -same- forward pass operation from _scratch_ to see how the forward-pass is accomplished.\n",
    "\n",
    "For this, we take the matrix of weights connecting the input layer to the ouput layer and perform a matrix multiplication operation using the `dot()` function and then add the bias weights for each of the output units in as well. We are therefore performing the weighted-sum for each of the output units _at the same time_ using these tools from linear algebra.\n",
    "\n",
    "After we perform the net input calculations, we will apply the logistic sigmoid activation function to produce the _same_ outputs as `keras` above. Let's try it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.008392 , 2.9025056, 6.789412 ],\n",
       "       [2.5750718, 4.3462625, 8.778631 ],\n",
       "       [2.5528216, 4.5434318, 9.582269 ],\n",
       "       [2.1073325, 3.3471422, 7.138145 ],\n",
       "       [2.59643  , 1.7730956, 5.0947795]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Net Inputs - from scratch\\\n",
    "## Weighted-sums + bias\n",
    "output_layer_neti = np.dot(np.float32(X[0:5,:]),\n",
    "                           model.get_weights()[0])+model.get_weights()[1]\n",
    "output_layer_neti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to keep in mind that `get_weights()` returned a python list with both a weight matrix (input units, 4, to output units, 3, so 4x3 for all $w_{ij}$) and a vector (output units, 3, so a vector of length 3 for the bias weights or each $w_{0}$). This means that `get_weights()[0]` was obtaining the weight matrix, and `get_weights()[1]` was obtainining the bias weights, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8816754 , 0.94797015, 0.9988757 ],\n",
       "       [0.92923987, 0.9872105 , 0.999846  ],\n",
       "       [0.92776287, 0.98947513, 0.9999311 ],\n",
       "       [0.89161384, 0.96601117, 0.9992065 ],\n",
       "       [0.93063146, 0.85484225, 0.99390864]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Ouput Layer Activations - from scratch\n",
    "## Logistic Sigmoid\n",
    "1.0 / (1.0 + np.exp(-1.0 * output_layer_neti))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the outputs produced by this operation are an **exact** match to those above, so we can see that `keras` is clearly implementing the same kind of neural networks explored so far in lecture.\n",
    "\n",
    "We will now move on to training up the network in order to change the weights using the Delta-rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta-rule learning from scratch\n",
    "\n",
    "Now that we have seen how to perform a forward-pass or _prediction_ using the neural network from scratch, let's perform a _learning_ step in the same manner. Above, we first used `keras` to perform the prediction and then used `numpy` to validate the learning equations. We will do something similar here, but in the opposite order. First, we will start by performing the prediction or forward-pass and then the corresponding backward-pass (Delta-rule learning step) from scratch. We will utilize a single training vector from $\\boldsymbol{X}$ and it's corresponding target vector from $\\boldsymbol{Y}$. The target vector will be used to calculate the _error_ of the prediction and the gradient of the error with respect to each weight (and bias weight) will be calculated, producing the updated weight matrix _from scratch_ using `numpy`. Second, we will perform a single learning step using `keras` and see if the weights updates to the model are the same as those that we performed from scratch.\n",
    "\n",
    "To begin, let's first store a copy of the weights from the model _as they currently are_. We will compute weights updates later, and then apply change to this _copy_ to simplify the code a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.09391999 -0.40792686  0.35855067]\n",
      " [ 0.5989735   0.703128    0.5537844 ]\n",
      " [-0.23786509  0.6430185   0.728176  ]\n",
      " [ 0.6450845   0.71855295  0.31226158]]\n",
      "[0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "## The weight matrix (from input layer to output layer)\n",
    "weights = model.get_weights()[0]\n",
    "## The bias weight vector (output layer)\n",
    "bias_weights = model.get_weights()[1]\n",
    "print(weights)\n",
    "print(bias_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can perform a forward-pass just like before to produce a prediction..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.0083919, 2.9025054, 6.789412 ]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_neti = np.dot(np.float32(X[0:1]),weights)+bias_weights\n",
    "output_neti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8816753 , 0.94797015, 0.9988757 ]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_acts = 1.0 / (1.0 + np.exp(-output_neti))\n",
    "output_acts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can verify that we have calculated the equations from class and that they match the way that `keras` performs these operations... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.88167536, 0.94797015, 0.9988756 ]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to test our prediction and update the weights to _improve_ that prediction in the future. First, we will calculate the _error_ or difference between the output predictions that the network is currently producing and the _target_ vector that it _should_ (or that we want it to) produce. This vector is just the first row in the target matrix, $\\boldsymbol{Y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.8816753 , -0.05202985,  0.9988757 ]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = output_acts - np.float32(Y[0:1])\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the derivation for mean-squared-error, MSE (which is a little different from the sum-squared-error, SSE, function we explored in lecture), we don't first multiply the error by 1/2 to keep the math a little more succinct. Therefore, in the final updates, the derivative is multiplied by 2. Still, the rest of the derivative with respect to the net inputs of the output units is just the _derivative_ of the sigmoid activation function at those net input values that we calculated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.20864782, 0.09864549, 0.0022462 ]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deriv = 2.0 * np.exp(-output_neti) / np.power(1.0+np.exp(-output_neti),2.0)\n",
    "deriv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to calculate our $\\delta$ values for the output units, but again MSE is a little different from SSE in that we also need to divide by the number of output units since we are minimizing the _mean_ of the squared errors. These $\\delta$ terms will be used update both the connection weights and bias weights appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06131988, -0.00171084,  0.00074789]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## For mean-squared-error loss, the math indicates\n",
    "## to normalize by 1/N where N is the number of\n",
    "## output units across which we are obtaining \n",
    "deltas = error*deriv*(1.0/len(bias_weights))\n",
    "deltas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `outer()` function to compute a 4x1 input vector multiplied by a 1x3 delta vector to make a 4x3 matrix which holds the results of the delta on the output layer times the activation on the input layer for each connection weight. We will update the weights by _subtracting_ this value from the weight matrix since this will result in _gradient descent_ in the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.3556553 , -0.00992285,  0.00433777],\n",
       "       [ 0.16556367, -0.00461926,  0.00201931],\n",
       "       [ 0.23914754, -0.00667226,  0.00291678],\n",
       "       [ 0.07358386, -0.002053  ,  0.00089747]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_updates = np.outer(np.float32(X[0:1]),deltas)\n",
    "w_updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we apply the updates, we first need to set our learning rate ($\\eta$) and multiply the updates by this value just like we did in lecture, and as was specified using the `lr` argument to the `SGD` optimizer that we chose above when compiling the model using `keras`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Learning rate\n",
    "eta = np.float32(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two cells show the _before_ and _after_ for the backward-pass. If all went well, the _after_ values should match the weights from the model after we perform a similar one-input pattern training step below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09391999, -0.40792686,  0.35855067],\n",
       "       [ 0.5989735 ,  0.703128  ,  0.5537844 ],\n",
       "       [-0.23786509,  0.6430185 ,  0.728176  ],\n",
       "       [ 0.6450845 ,  0.71855295,  0.31226158]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09036344, -0.40782762,  0.35850728],\n",
       "       [ 0.5973179 ,  0.7031742 ,  0.55376416],\n",
       "       [-0.24025656,  0.6430852 ,  0.72814685],\n",
       "       [ 0.6443487 ,  0.71857345,  0.3122526 ]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Weights after delta-rule update\n",
    "## Subtract to minimize error (gradient descent)\n",
    "weights - eta*w_updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same idea can be used for the bias weights, but remember that these are always fully active, so they just use 1.0 times the delta value for their updates. Again, we see a _before_ and _after_ snapshot of what we expect from `keras` according to the principle of gradient descent in MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-6.1319879e-04,  1.7108367e-05, -7.4789200e-06]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Bias weights after delta-rule update\n",
    "## Subtract to minimize error (gradient descent)\n",
    "bias_weights - eta*deltas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're ready to validate our approach and see if `keras` produces the same result as our calculated weight updates above.\n",
    "\n",
    "Let's ask `keras` to perform a single _epoch_ of training on a data set with a single input-target pair. Each epoch is a complete pass through the training data, and the batch size determines how many pattern gradients will be summed together before performing the weight update (to produce a smoother descent in error, use larger batch sizes). However, we are just testing our model here, so we will just do a single pass on one pattern, so it makes sense to set the batch size to 1 (it would ignore any larger number anyway since there isn't more training data provided).\n",
    "\n",
    "Now for the moment of truth..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.09036344, -0.40782762,  0.35850728],\n",
       "        [ 0.5973179 ,  0.7031742 ,  0.55376416],\n",
       "        [-0.24025656,  0.6430852 ,  0.72814685],\n",
       "        [ 0.6443487 ,  0.71857345,  0.3122526 ]], dtype=float32),\n",
       " array([-6.1319885e-04,  1.7108367e-05, -7.4790328e-06], dtype=float32)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = model.fit(X[0:1],Y[0:1],batch_size=1,epochs=1,verbose=0)\n",
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we performed the weight update to our model using the `fit()` function. This is the training function provided by `keras` for adapting weights in our neural networks, and we will see how to use it in a more general way below. However, after this single forward-backward pass, we can now print the new weights from the model, and you can see that they are now a __nearly exact__ match to our calculated/updated weights using `numpy` (some small rouding errors on the weights will be within an acceptable tolerance for floating point operations such as these - different approximations are used to improve performance across different libraries). Therefore, in principle we could design our networks from scratch in this way all of the time. However, you can clearly now appreciate how simple the `keras` framework makes performing these operations. This is especially true when you need an architecture that combines many different types of layers and other options that we will explore in the future.\n",
    "\n",
    "Now that we at least understand in principle how these nets are computing their results, let's see how we might use them to learn something about the Iris data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Single-Layer Network\n",
    "\n",
    "Time to get training! First, select a batch size for the stochastic gradient update: the number of patterns experienced between weight updates. Second, choose the number of epochs (complete passes through the data) that you would like to peform. Third, select a certain fraction of the data that you would like to use for _validation_ of your training results (0.5 would mean that 50% of the data is not used for training, but instead used to test for _generalization_).\n",
    "\n",
    "We will utilize the `fit()` member function of our model for perfoming the training, which accepts these three options to control its behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.5805 - accuracy: 0.3333 - val_loss: 0.5731 - val_accuracy: 0.3333\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5737 - accuracy: 0.3333 - val_loss: 0.5652 - val_accuracy: 0.3333\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5659 - accuracy: 0.3333 - val_loss: 0.5562 - val_accuracy: 0.3333\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5569 - accuracy: 0.3333 - val_loss: 0.5456 - val_accuracy: 0.3333\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5464 - accuracy: 0.3333 - val_loss: 0.5338 - val_accuracy: 0.3333\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5344 - accuracy: 0.3333 - val_loss: 0.5202 - val_accuracy: 0.3333\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5211 - accuracy: 0.3333 - val_loss: 0.5055 - val_accuracy: 0.3333\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5063 - accuracy: 0.3333 - val_loss: 0.4896 - val_accuracy: 0.3333\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.4906 - accuracy: 0.3333 - val_loss: 0.4734 - val_accuracy: 0.3333\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.4748 - accuracy: 0.3333 - val_loss: 0.4580 - val_accuracy: 0.3333\n"
     ]
    }
   ],
   "source": [
    "# Basic training parameters\n",
    "batch_size = 16\n",
    "epochs = 10\n",
    "validation_split = 0.5\n",
    "\n",
    "# Train the model and record the training\n",
    "# history for later examination\n",
    "history = model.fit(X, Y,\n",
    "          batch_size = batch_size,\n",
    "          epochs = epochs,\n",
    "          verbose = 1,\n",
    "          validation_split = validation_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that you will get some output for each epoch that you train the network, indicating progress through the training. You can turn **off** this output by using the `verbose=0` option at any time. This is sometimes useful since there are other ways that we can look at the training performance using the `history` data that came back from the fitting process.\n",
    "\n",
    "You can see that the loss values were **decreasing** (error was going down), even if **accuracy** wasn't necessarily increasing. To make this network perform better we could:\n",
    "1) Increase the number epochs used in the training process\n",
    "2) Increase the learning rate on the stochastic gradient optimizer\n",
    "3) Rebuild the network starting from our `model = keras.Sequential()` statement to initialize the weights at a better starting location in the weight space.\n",
    "3) Other things that we will explore at a later time (**don't use any other tricks for this assignment**)...\n",
    "\n",
    "Let's plot the history information for a moment to see what happened across training. This is just a graphical depiction of what happened during the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(1)  \n",
    "   \n",
    "# summarize history for accuracy  \n",
    "   \n",
    "plt.subplot(211)  \n",
    "plt.plot(history.history['accuracy'])  \n",
    "plt.plot(history.history['val_accuracy'])  \n",
    "plt.title('model accuracy')  \n",
    "plt.ylabel('accuracy')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'val'], loc='upper left')  \n",
    "   \n",
    "# summarize history for loss  \n",
    "   \n",
    "plt.subplot(212)  \n",
    "plt.plot(history.history['loss'])  \n",
    "plt.plot(history.history['val_loss'])  \n",
    "plt.title('model loss')  \n",
    "plt.ylabel('loss')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These graphical reports (after using `verbose=0`) will be very useful for completing this assignment. Use the code above as a template for constructing your graphs. Limited changes should be needed to complete this assignment.\n",
    "\n",
    "However, let's see how our network performs now on the entire data set. (This is not the best way to assess performance: we should use a **testing data set** with examples that the network has never seen **and** that we have never used to tune our hyperparameters. So, the evaluation below is a **biased** performance evaluation. However, these are small data sets so there isn't much data available for doing this kind of analysis currently: *we will use proper techniques in future assignments*.) Now that we have trained the network, we can use the `evaluate()` method to determine this information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 1ms/step - loss: 0.4617 - accuracy: 0.3333\n",
      "Test loss: 0.4617256820201874\n",
      "Test accuracy: 0.3333333432674408\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X, Y, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, we are only getting 33% of the _validation_ examples classified correctly! However, we can use one (or more) of the three suggested tricks above for improving the performance of the network. Unless you rebuild the model again from scratch, training can be carried over from previous `fit()` operations. So, **if** we ran `fit()` *again* now for another 10 epochs, it would be **20 epochs total of training**. However, the history information from the previous 10 epochs may be lost, so be sure to keep records of the training process or restart from scratch if you want to train for more epochs from the very beginning\n",
    "\n",
    "Some models _also_ take a long time to evaluate, so the `verbose()` option is available to help determine how long this process takes to complete, but feel free to turn it off by setting it to zero.\n",
    "\n",
    "**If you want to get a fresh model** with new initial weights and no experience with the problem (sometimes needed due to optimization errors or bad hyperparameter choices in your models), then you will need to rerun all model construction/compilation steps: start back at `model = keras.Sequential()`. This will ensure that you get a freshly constructed model for subsequent training/validation and ensure that any hyperparameter changes have taken full effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice!\n",
    "\n",
    "Now that you have experienced the *process* for creating a single-layer network, try adjusting the suggested parameters above to learn how to build a single-layer neural network to classify the Iris data set!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
