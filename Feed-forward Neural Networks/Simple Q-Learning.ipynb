{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning with Feed-forward Neural Networks\n",
    "\n",
    "Reinforcement learning covers a broad range of problems where we would like to provide only a sparse feedback signal (in either spatial or temporal terms) and still have an agent learn to solve the problem of interest. Such problems are of the sort where manually labeling feedback (such as used for supervised learning) would be too cumbersome or time-consuming. Often, it is easy to describe situations which are \"good\" or \"preferable\" in some way rather than enumarating all of the steps needed to create such situations. We typically quantify these preferences in terms of a *scalar reward* which describes the level of \"goodness\" (positive) or \"badness\" (negative) with zero being an essentially neutral condition.\n",
    "\n",
    "For this example, we will utilize a feed-forward network to learn the so-called *Q-function* for a reinforcement learning agent which learns to solve a simple task while being provided a relatively simple, well-defined reward profile. The *Q-function* is an estimate of the *learned* value of state-action pairs, which can be leveraged for proper action selection and maximization of the *sum of discounted future rewards* that the agent receives. While the Q-function is often represented by an array which enumerates all possible state-action pairs and encodes the so-called Q-values for all such pairs, it is far more preferable to utilize a neural network for storing these values (learning the Q-function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Cart-Pole Problem\n",
    "\n",
    "We will be solving the classic cart-pole problem borrowed from the OpenAI Gym framework. OpenAI Gym is a good resource for learning how to program reinforcement learning agents since the gym provides a set of problem environments with a consistent, common interface. This allows us to focus on the development of the agent instead of the development of the environment. Nevertheless, some familiarity with the environment will be needed to start building the agent. One other interesting property of these environments is that they can be rendered to provide a snapshot of what the agent is experiencing which we can visualize as a third-party observer. The agent will only be provided a set of *features* from the environment for this problem, so the visualizations are just for our own learning/validation purposes.\n",
    "\n",
    "First, let's initialize the gym and the cart-pole environment, and visualize what the environment looks like at the start of a training *episode*..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI Gym\n",
    "import gym\n",
    "\n",
    "# Rendering tools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03369208, -0.02870218,  0.00063917, -0.04343481])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f09449e7b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the cart-pole environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Reinitialize the environment for an episode\n",
    "observation = env.reset()\n",
    "\n",
    "# Look at the features the agent will observe during training...\n",
    "display(observation)\n",
    "\n",
    "# Render the scene for our visualization purposes...\n",
    "plt.imshow(env.render(mode='rgb_array'))\n",
    "plt.title(\"New Episode\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The black box at the bottom represents a cart which can be pushed to the left or right along the black line without friction. There is a pole balanced at the center of the cart which will begin to fall either to the left or right depending on the small amount of displacement of the pole's angle from 90 degrees. The agent will be allowed to apply a small force in either the left or right direction in an attempt to compensate for the falling pole in an attempt to balance it on the cart. For every timestep where the agent can keep the pole within 12 degrees of fully upright, the agent will be rewarded (r=1). We will also manually punish the agent by a larger amount (r=-10) when it \"drops the pole\" to more explicitly discourage poor performance (instead of only encouraging good performance). However, the agent will initially start untrained and will not be very good at understanding how to use the available actions to make sure that it experiences many rewards and avoids punishment. Also, the agent is provided with a list of features which correspond to the configuration of the environment for making such decisions:\n",
    "\n",
    "| Index | Feature | Minimum | Maximum |\n",
    "| --- | --- | --- | --- |\n",
    "| 0 | Cart Position | -2.4 | 2.4 |\n",
    "| 1 | Cart Velocity | -$\\infty$ | $\\infty$ |\n",
    "| 2 | Pole Angle | -41.8 | 41.8 |\n",
    "| 3 | Pole Velocity | -$\\infty$ | $\\infty$ |\n",
    "\n",
    "Older approaches to solving this problem have leveraged various methods to reencoding the input features into more informative forms, such as fuzzy bucket or thermometer encodings. However, more recent neural networks have been shown to be successful without having to reencode our input features which is both more intuitive and practical. Here we will provide this 4-element vector of features to represent the *state* of the environment for our network.\n",
    "\n",
    "Now that we have seen how to observe the environment, let's see if we can perform some actions (in this case completely random) to see if we can interact with the environment over time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Shape of the observation vectors\n",
    "display(env.observation_space.shape)\n",
    "\n",
    "# Number of possible actions\n",
    "display(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f090c1e1e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get some random tools :)\n",
    "import random\n",
    "\n",
    "# Initialize the environment and perform 10 random actions\n",
    "env.reset()\n",
    "reward = 0.0\n",
    "for time in range(10):\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Time: %d | Reward: %f\" % (env.spec.id, time, reward))\n",
    "    clear_output(wait=True)\n",
    "    display(plt.gcf())\n",
    "    observation, reward, done, info = env.step(random.choice(range(env.action_space.n))) # take a random action\n",
    "    if (done):\n",
    "        break\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's unpack this a little...\n",
    "\n",
    "First, we `reset()` the environment object to set up the initial conditions of the episode. This is basically with the pole *very nearly* straight. We then loop over 10 iterations of interaction with the environment (one iteration is a *time step*). One each time step we will first, `render()` the current state and plot the corresponding image. In order to to this progressively (i.e. so there are not 10 different images displayed as output for the cell) we will then call `clear_output()` which was imported from `IPython.display`. This resets the output buffer even if a prior image was sent to the browser, hence we use the `wait=True` option to ensure rendering was over before clearing it away. Then we display the current plot (note that `plt.show()` is not being used) using the `display(plt.gcf())` command. In this way, on each time step the resulting plot will be rendered on top of the last plot, resulting in a \"movie\" of the output.\n",
    "\n",
    "On each time step, we utilize the `step()` function to both indicate which action we would like to perform (`random.choice(range(env.action_space.n)))`) and then collect the new state that results from taking this action (`observation`) along with additional data (r=`reward`, `done`, and `info`). We will not use the `info` field for working with this environment, but other environments may report additional information besides the other three (which are returned by default). The `reward` is the scalar reward value obtained from visiting the new state, and the `done` flag indicates whether the cart has moved beyond the boundaries of the task or beyond the 12 degree range allowed. It will be `False` until one of these two termination conditions is met when it will then be returned as `True`.\n",
    "\n",
    "The size of the action space for the problem is 2 since there are only two actions: apply force to the left or apply force to the right. We are just sampling randomly from these two actions on each step, so there is no intelligence in this agent. However, it should now be clear how the agent can be programmed to interface with the environment. All of our learning will consist in deciphering which action would need to be performed given the current state of the environment.\n",
    "\n",
    "We will come back to visualization of the task in this way later on when testing our agent's performance, but for now we will simple use the 4-element observation vector, 2 available actions, and scalar reward signal to train up a Q-learning agent to solve the problem.\n",
    "\n",
    "Our first step along this path is to create a neural network model which tried to predict the Q-value associate with state-action pairs, $Q(s,a)$. The network will receive the obsevation vector ($s$) as input, and then produce a $Q(s,a)$ for *each action*. Therefore, we will build a network with 4 input units and 2 output units. Q-values are also real-valued, so we will utilize the `linear` activation function for the output layer, similar to regression problems. Let's set up the network now, and see how we might use it to select an action for our agent to perform..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 20)                100       \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 2)                 42        \n",
      "=================================================================\n",
      "Total params: 562\n",
      "Trainable params: 562\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "# A simple multilayer architecture....\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(20, input_shape=[env.observation_space.shape[0]], activation='relu'))\n",
    "model.add(keras.layers.Dense(20, activation='relu'))\n",
    "model.add(keras.layers.Dense(env.action_space.n, activation='linear'))\n",
    "model.compile(loss='mse',optimizer=keras.optimizers.Adam())\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network will be used to calculate/store the Q-function. Since we prefer to select actions that have high value (highest Q), we will present an state vector (`observation`) to the network and utilize the network outputs as predictions of the corresonding Q-values for the two state-action pairs that we are considering at the current time step. While we completed 10 time steps above randomly selecting our actions, instead the network will provide us with Q-values for making the decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.034091  ,  0.56614125]], dtype=float32)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn our observation vector into a matrix of observations\n",
    "# with only -one- observation and run predict()\n",
    "Q = model.predict(np.expand_dims(observation,axis=0))\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the predicted Q-values for the \"left\" and \"right\" actions for this environment. While the agent is using a randomly initialized network, and so will generally not produce reasonable Q values just yet, we will see this converge to the *sum of discounted rewards* desired under Q-learning theory. We will cover that more later, but for now let's see how we use that information to make a decision about how to act.\n",
    "\n",
    "The general idea is to prefer actions with the larger Q values, so generally we will select the action (either 0 for left of 1 for right) with the higher value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the action with the higher Q-value is easily selected using the `argmax()` function. This is the idea behind **exploitation** in reinforcement learning: we utilize our learned knowleged of Q-values to make decisions on actions to perform. However, it doesn't explain how we should encourage **exploration** where we sometimes *do not* follow our Q-function. A common strategy is to employ an $\\epsilon$-soft action selection policy. In this kind of policy, we will select our action using `argmax()` above, **except** for some fraction of the time steps, $\\epsilon$, when we instead choose a *random action* just like our initial exploration with such actions earlier. We usually keep $\\epsilon$ small so that these are *rare* events, but we can also treat it like any other hyperparameter and *anneal* it over time. In this case $\\epsilon$ starts high (1) and is then decayed to a small value over time (usually no lower than a small fraction, for example 0.01)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example e-soft policy...\n",
    "epsilon = 0.5 # Half of the time, a random action is chosen...\n",
    "action = random.choice(range(env.action_space.n)) if random.random() < epsilon else np.argmax(Q)\n",
    "action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's hook the agent into the environment, and see what happens..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f08747afba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the environment and let the agent decide!\n",
    "observation = env.reset()\n",
    "epsilon = 0.0 # No exploration!\n",
    "reward = 0.0\n",
    "for time in range(30):\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Time: %d | Reward: %f\" % (env.spec.id, time, reward))\n",
    "    clear_output(wait=True)\n",
    "    display(plt.gcf())\n",
    "    Q = model.predict(np.expand_dims(observation,axis=0)) # Compute Q\n",
    "    action = random.choice(range(env.action_space.n)) if random.random() < epsilon else np.argmax(Q)\n",
    "    observation, reward, done, info = env.step(action) # take action!\n",
    "    if (done):\n",
    "        break\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so the agent can easily be used to control the cart, but it currently can't do a very good job since we have not learned the correct Q-values for this environment. We will need to program up the learning part next. There are a few moving parts here that we need to put into place:\n",
    "\n",
    "1. Replay memory storage\n",
    "2. Target Q-function model\n",
    "3. Replay learning function\n",
    "\n",
    "First, let's talk a little about *replay memory*. While we could perform Q-learning on each time step and make the network learn *on-line* while perform the task, this leads to problems with training neural networks which require folding in many training examples from a variety of state-action pairs in order to converge to the correct Q-function. The reason for this is because there is so much information shared between successive time steps on many reinforcement learning tasks. That is, the new state will \"*look*\" very similar to the last state we just came from since the actions don't cause drastic changes in observations. Even if drastic changes *were* made, the sequence of states that are experience are *temporally* similar (as in, they occur at similar points in time) so that the network is more focused on the most temporally and spatially similar observations while learning. This will lead the network to become focused more on the present learning scenarios and forget (or more correctly, overwrite) learned Q-values from past or even future observations that it has (or will) encounter. Therefore, we will retain a memory of the last, say 10000, state transitions and then draw a random sample from this memory bank for training. In this way, we will be drawing observations which are neither spatially nor temporally similar (on average) and the network will then be able to leverage learning from a wide range of experiences instead of the most current only.\n",
    "\n",
    "In actuality, we will let the network complete an episode, and then run through several batches of observations from the replay memory to train the Q-function. We will then return to complete another episode, followed by another round of replay training. This *replay memory* is **critical** to the success of a good reinforcement learning agent so allocating a significant amount of storage space and heavily leveraging many random samples from it is the **key** technique to designing a successful reinforcement learning agent using a neural network. Research is on-going on ways to mitigate the size and training epochs of such a memory for more practical use in memory-intensive applications.\n",
    "\n",
    "Second, we will utilize the *target function* network strategy. While we are running replay memory training, we will be changing Q values (updating the function). However, the very network that is generating our Q-values for the current time step is also generating our Q-values for the next time step (and so on). We also utilize the *difference* between these subsequent Q-values to update the network (to set our target Q-values). This means when we update a Q-value (and share weights with other Q-values) we inadvertently update other Q-values as well. There is a *moving target* problem here where the network is trying to speed up with or slow down with it's own Q-value updates! This can destabilize the learning capabilities of a neural network similar to how the gradients explode or disapper in a recurrent neural network (since you are reusing the same weights to calculate the predictions and targets, it forms a feedback loop). We can eliminate this instability (to a large degree) by making a copy of our current network Q-network, and only using this copy to generate targets. This means that the *target network* isn't trained, but only used to create the targets for the main model. These targets will therefore be fixed until we are finished training our main Q-function model. Then, we can just occasionally copy over the weights from our trained network into the target network again before the next round of training. While not as critical as the replay memory above, using a *target model* makes for a *drastically easier training experience* by largely eliminating the moving target problem.\n",
    "\n",
    "Finally, we just need to stitch together the *replay memory* and *target model* to form the replay memory training function which will train the model to learn from the replay memory for a specified number of randomly sampled batches, and then update the target function at the end..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Replay Memory\n",
    "class ReplayMemory:\n",
    "    def __init__(self, memory_size, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.size = 0\n",
    "        self.maxsize = memory_size\n",
    "        self.current_index = 0\n",
    "        self.current_state = np.zeros([memory_size,env.observation_space.shape[0]])\n",
    "        self.action = [0]*memory_size # Remember, actions are integers...\n",
    "        self.reward = np.zeros([memory_size])\n",
    "        self.next_state = np.zeros([memory_size,env.observation_space.shape[0]])\n",
    "        self.done = [False]*memory_size # Boolean (terminal transition?)\n",
    "\n",
    "    def remember(self, current_state, action, reward, next_state, done):\n",
    "        # Stores a single memory item\n",
    "        self.current_state[self.current_index,:] = current_state\n",
    "        self.action[self.current_index] = action\n",
    "        self.reward[self.current_index] = reward\n",
    "        self.next_state[self.current_index,:] = next_state\n",
    "        self.done[self.current_index] = done\n",
    "        self.current_index = (self.current_index+1)%self.maxsize\n",
    "        self.size = max(self.current_index,self.size)\n",
    "    \n",
    "    def replay(self, model, target_model, num_samples, sample_size, gamma):\n",
    "        # Run replay!\n",
    "        \n",
    "        # Can't train if we don't yet have enough samples to begin with...\n",
    "        if self.size < sample_size:\n",
    "            return\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            # Select sample_size memory indices from the whole set...\n",
    "            current_sample = random.choice(range(self.size),sample_size,replace=False)\n",
    "            \n",
    "            # Slice memory into training sample\n",
    "            current_state = self.current_state[current_sample,:]\n",
    "            action = self.action[current_sample]\n",
    "            reward = self.reward[current_sample]\n",
    "            next_state = self.next_state[current_sample,:]\n",
    "            done = self.done[current_sample,:]\n",
    "            \n",
    "            # Obtain model's current Q-values\n",
    "            model_targets = model.predict(current_state)\n",
    "            \n",
    "            # Obtain target Q-values from target network...\n",
    "            targets = reward + gamma*\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could wrap this in a class structure for each usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(range(2)) if random.random() < epsilon else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
