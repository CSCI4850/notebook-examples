{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Sequence-to-Sequence Mappings\n",
    "\n",
    "Many times, we want to map an arbitrary length sequence to another arbitrary length sequence for a machine learning task. The most common example would be for machine translation where the number of characters/words used to encode a sentence in one language rarely matches with the number of characters/works used to encode a sentence with the same meaning in another language. Thus, the many-to-many mapping used by the simple recurrent network used in the previous example will not be approprite.\n",
    "\n",
    "In order to accomplish this task, the most common architecture used at this time is the *encoder-decoder* framework. First, we build an *encoder* network which takes an arbitrary length input sequence, and encodes a distributed representation of that input sequence using the hidden layer of a recurrent network. This is similar to a standard many-to-many recurrent network, except there is no output layer for the encoder. Instead, the input sequence information is simply folded into the hidden layer activation pattern at each time step. Once the encoder finishes generating a *gestalt* representation of the input sequence (using a recurrent hidden layer), the hidden layer activations are fed into a *decoder* network which translates the *gestalt* representation into the corresponding output sequence using another recurrent hidden layer and time-distributed output layer. Special *start* and *stop* patterns are used to encode for the starting and ending points of target sequences during training, so the *decoder* network learns to indicate when it is ready to start and finish a particular output sequence. Using its two subnetworks, the encoder-decoder framework forms a general sequence-to-sequence learning framework.\n",
    "\n",
    "In order to illustrate how to build and train an encoder-decoder network, we will utilize the parity task from the simple recurrent network example. Even though the sequence lengths are the same for both the input and output for this task, we can still utilize it to illustrate the core concepts involved. Also, we will be using long short-term memory for the recurrent layers of these networks to show their effectiveness at learning these kinds of tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import what we need for the job...\n",
    "import keras\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "# Visualization\n",
    "from IPython.display import SVG\n",
    "from IPython.display import display\n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the parity problem again, but we need to encode the input and target sequences a bit more generically this time to show how the process might be generalized to other types of sequence processing domains.\n",
    "\n",
    "One key idea is that we need to map our possible sequence elements into a one-hot encoding dictionary which will be useful for arbitray sequence specification for both the encoder and decoder parts of the network. We will use the following mapping:\n",
    "\n",
    "Input Sequences:\n",
    "\n",
    "| Symbol | One-hot Encoding |\n",
    "| --- | --- |\n",
    "| `0` | `10` |\n",
    "| `1` | `01` |\n",
    "\n",
    "Ouput Sequences:\n",
    "\n",
    "| Symbol | One-hot Encoding |\n",
    "| --- | --- |\n",
    "| `0` | `1000` |\n",
    "| `1` | `0100` |\n",
    "| start | `0010` |\n",
    "| stop  | `0001` |\n",
    "\n",
    "This encoding will necessarily make the problem easier to solve (0 is now mapped to a meaningful orthogonal dimension to the other symbols), but more importantly, it shows how each symbol in the input/output sequences needs an orthogonal dimension in the encoded form. Other distributed codes may also work, but high dot product similarity between vectors in the encoding chosen may affect the network's learning capabilities in unpredictable ways (more than likely negative). Also, it should be clear here that we don't even need to have the same *symbols* in the encoder and decoder networks. They can be arbitrary mappings in both time and space, which makes this a powerful, flexible approach.\n",
    "\n",
    "OK, so let's generate a large set of training and testing sequences for the network to experience on this task..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of sequences to model\n",
    "length = 10\n",
    "\n",
    "# Note we are making these the same, but they don't -have- to be!\n",
    "input_length = length\n",
    "output_length = length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a good rule of thumb, we just want to create a network and train it to learn just a single example first. After that, we can extend it to a larger data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a random string of 0s and 1s\n",
    "x_train = np.round(np.random.uniform(0,1,[length])).reshape([1,length,1])\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate parity (note this is the same algorithmic approach\n",
    "# that we are trying to encourage our net to learn!)\n",
    "def parity(x):\n",
    "    temp = np.zeros(x.shape)\n",
    "    mem = False\n",
    "    # Iterate over the sequence\n",
    "    for i in range(x.shape[0]):\n",
    "        if x[i,0] > 0.5:\n",
    "            current = True\n",
    "        else:\n",
    "            current= False\n",
    "        mem = np.logical_xor(mem,current)\n",
    "        if mem:\n",
    "            temp[i,0] = 1.0\n",
    "        else:\n",
    "            temp[i,0] = 0.0\n",
    "    return (temp.reshape(1,temp.shape[0],temp.shape[1])) # Tensor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.],\n",
       "        [ 0.],\n",
       "        [ 1.],\n",
       "        [ 0.],\n",
       "        [ 1.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = parity(x_train[0,:,:])\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn these into a set of encoded input-output vectors using the transformation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  1.],\n",
       "        [ 0.,  1.],\n",
       "        [ 0.,  1.],\n",
       "        [ 0.,  1.],\n",
       "        [ 0.,  1.],\n",
       "        [ 0.,  1.],\n",
       "        [ 1.,  0.],\n",
       "        [ 1.,  0.],\n",
       "        [ 1.,  0.],\n",
       "        [ 1.,  0.]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.expand_dims(keras.utils.to_categorical(x_train[0,:,0]),axis=0)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  0.,  1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.expand_dims(np.vstack([np.array([0,0,1,0]),\n",
    "                   np.hstack([keras.utils.to_categorical(y_train[0,:,0]),np.zeros([input_length,2])]),\n",
    "                   np.array([0,0,0,1])]),axis=0)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, note the shapes of the sequences..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10, 2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 12, 4)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to divide the target data into two groups which correspond to the input-target pairs for the *decoder* network. In essence, the decoder network is really just a many-to-many recurrent network, and will be trying to predict the next part of the target sequence (`postY`) given *both* the encoder output representation *and* the previous target sequence entry (`preY`). This previous target sequence entry is not strictly necessary, but using it represents a training strategy called *teacher forcing* which makes it easier to learn some sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12 time steps total (10 + start + stop)\n",
    "# Divide into two groups of 11\n",
    "# First from 0 ... 10\n",
    "preY = Y[:,0:output_length+1,:]\n",
    "# Second from 1 ... 11\n",
    "postY = Y[:,1:output_length+2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 11, 4)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 11, 4)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postY.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's make an encoder-decoder framework for learning this sequence. We need to use the Keras *functional* API for building this network as well. To create networks of this kind, we first create `layer` objects and then call their `()` functions to make them take inputs from the provided layers' outputs. This allows for networks to receieve input from more than one layer at a time, or pass their outputs to more than one layer at a time. Once all of the layers have been connected in the desired ways, then we create a `Model` object which is provided a set of input and output layers for the overall model. This allows for more general network topologies to be constructed than can be achieved with the *sequential* API.\n",
    "\n",
    "Generally, the process looks like so:\n",
    "1. mylayer = Layer(...)\n",
    "2. mylayer_output(s) = mylayer(input_layer(s))\n",
    "\n",
    "Outputs from a layer may be passed as inputs to other layers for chaining layers together..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           (None, None, 2)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           (None, None, 4)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_14 (LSTM)                  [(None, 20), (None,  1840        input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_15 (LSTM)                  [(None, None, 20), ( 2000        input_18[0][0]                   \n",
      "                                                                 lstm_14[0][1]                    \n",
      "                                                                 lstm_14[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, None, 4)      84          lstm_15[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,924\n",
      "Trainable params: 3,924\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Size of the gestalt, context representations...\n",
    "hidden_size = input_length*2\n",
    "\n",
    "## Encoder Construction\n",
    "\n",
    "# Make the layers\n",
    "encoder_input = keras.layers.Input(shape=(None, X.shape[2]))\n",
    "encoder_hidden = keras.layers.LSTM(hidden_size, return_state=True)\n",
    "# Tie the hidden layer to the input layer (passed in) \n",
    "encoder_output, enc_state_h, enc_state_c = encoder_hidden(encoder_input)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [enc_state_h, enc_state_c]\n",
    "\n",
    "## Decoder Construction\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_input = keras.layers.Input(shape=(None, Y.shape[2]))\n",
    "decoder_hidden = keras.layers.LSTM(hidden_size, return_sequences=True, return_state=True)\n",
    "# Connect hidden to input (also reads from the encoder...)\n",
    "decoder_hidden_output, decoder_state_h, decoder_state_c = decoder_hidden(decoder_input,\n",
    "                                                                         initial_state=encoder_states)\n",
    "decoder_output = keras.layers.Dense(Y.shape[2], activation='softmax')\n",
    "# Connect output to hidden\n",
    "decoder_output = decoder_dense(decoder_hidden_output)\n",
    "\n",
    "# Our functional API model now has -two- input layers:\n",
    "# 1. Reads from X\n",
    "# 2. Reads from preY\n",
    "# and has a -single- output layer...\n",
    "# 1. Targets are postYY\n",
    "model = keras.Model([encoder_input, decoder_input], decoder_output)\n",
    "\n",
    "# Compile it...\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"264pt\" viewBox=\"0.00 0.00 277.00 264.00\" width=\"277pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 260)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-260 273,-260 273,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140460804709960 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140460804709960</title>\n",
       "<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 132,-255.5 132,-219.5 0,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"66\" y=\"-233.8\">input_17: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140460804710016 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140460804710016</title>\n",
       "<polygon fill=\"none\" points=\"13.5,-146.5 13.5,-182.5 118.5,-182.5 118.5,-146.5 13.5,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"66\" y=\"-160.8\">lstm_14: LSTM</text>\n",
       "</g>\n",
       "<!-- 140460804709960&#45;&gt;140460804710016 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140460804709960-&gt;140460804710016</title>\n",
       "<path d=\"M66,-219.313C66,-211.289 66,-201.547 66,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"69.5001,-192.529 66,-182.529 62.5001,-192.529 69.5001,-192.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140460804710128 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140460804710128</title>\n",
       "<polygon fill=\"none\" points=\"137,-146.5 137,-182.5 269,-182.5 269,-146.5 137,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"203\" y=\"-160.8\">input_18: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140460803147144 -->\n",
       "<g class=\"node\" id=\"node4\"><title>140460803147144</title>\n",
       "<polygon fill=\"none\" points=\"81.5,-73.5 81.5,-109.5 186.5,-109.5 186.5,-73.5 81.5,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"134\" y=\"-87.8\">lstm_15: LSTM</text>\n",
       "</g>\n",
       "<!-- 140460804710128&#45;&gt;140460803147144 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140460804710128-&gt;140460803147144</title>\n",
       "<path d=\"M186.297,-146.313C177.737,-137.505 167.164,-126.625 157.769,-116.958\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"160.029,-114.261 150.549,-109.529 155.009,-119.14 160.029,-114.261\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140460804710016&#45;&gt;140460803147144 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>140460804710016-&gt;140460803147144</title>\n",
       "<path d=\"M82.4609,-146.313C90.897,-137.505 101.317,-126.625 110.575,-116.958\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"113.302,-119.172 117.691,-109.529 108.246,-114.33 113.302,-119.172\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140461158797208 -->\n",
       "<g class=\"node\" id=\"node5\"><title>140461158797208</title>\n",
       "<polygon fill=\"none\" points=\"83,-0.5 83,-36.5 185,-36.5 185,-0.5 83,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"134\" y=\"-14.8\">dense_3: Dense</text>\n",
       "</g>\n",
       "<!-- 140460803147144&#45;&gt;140461158797208 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>140460803147144-&gt;140461158797208</title>\n",
       "<path d=\"M134,-73.3129C134,-65.2895 134,-55.5475 134,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"137.5,-46.5288 134,-36.5288 130.5,-46.5289 137.5,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualization - wish we could see the recurrent weights!\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, these visualizations are a little helpful now, maybe? Well, again, the recurrent connection weights are stored in the LSTM layers, so there is little information there. However, it is clear how the *teacher forcing* layer (second input layer) is connected in and provides a little more support to the learning. We will feed the output from the final dense layer back into this layer when performing prediction below as well once it has learned the sequence properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c5ee8061cc48c6b35a0178fd9a358c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9556dd1ffe84c43a4cd79aa52020248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 0', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1/1 [==============================] - 0s 343ms/step\n",
      "Accuracy: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1   # only one pattern...\n",
    "epochs = 200\n",
    "history = model.fit([X,preY], postY,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=0,\n",
    "          callbacks=[TQDMNotebookCallback()])\n",
    "print('Accuracy:',model.evaluate([X,preY],postY)[1]*100.0,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbec848a208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)  \n",
    "# summarize history for accuracy \n",
    "plt.subplot(211)  \n",
    "plt.plot(history.history['acc'])  \n",
    "plt.title('model accuracy')  \n",
    "plt.ylabel('accuracy')  \n",
    "plt.xlabel('epoch')  \n",
    "# summarize history for loss  \n",
    "plt.subplot(212)  \n",
    "plt.plot(history.history['loss'])  \n",
    "plt.title('model loss')  \n",
    "plt.ylabel('loss')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.tight_layout()\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network appears to have learned the sequence, but there is one thing wrong with the final evaluation: the *teacher forcing* inputs are still bein provided on each time step. Instead, we should be feeding the *output* from the decoder back into this input layer, so that it can use *it's own predictions* to forecast the next step. If we had left off the teacher forcing inputs entirely, this would not have been necessary, but let's make a more proper test of the network now.\n",
    "\n",
    "In order to accomplish this part of the job, we will construct *another* model, but just use the layers already defined and trained before. This will allow us to connect the layers together to create a model which accomplishes the feedback loop for us..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1 - make the encoder\n",
    "\n",
    "# Make just a model out of the encoder\n",
    "# input = encoder_input (Input layer)\n",
    "# output = encoder_states (enc Hidden layer * 2)\n",
    "encoder_model = keras.Model(encoder_input, encoder_states)\n",
    "\n",
    "# Part 2 - make the decoder\n",
    "\n",
    "# Make just a model out of the decoder\n",
    "# input = encoder_states (enc Hidden layer * 2)\n",
    "# output = decoder_output\n",
    "decoder_state_input_h = keras.layers.Input(shape=(encoder_hidden_size,))\n",
    "decoder_state_input_c = keras.layers.Input(shape=(encoder_hidden_size,))\n",
    "# Connect hidden to input(s)\n",
    "decoder_states_input = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_hidden_output, decoder_state_h, decoder_state_c = decoder_hidden(decoder_input,\n",
    "                                                                         initial_state=decoder_states_input)\n",
    "decoder_states = [decoder_state_h, decoder_state_c]\n",
    "# Connect output to hidden(s)\n",
    "decoder_output = decoder_dense(decoder_hidden_output)\n",
    "decoder_model = keras.Model(\n",
    "    [decoder_input] + decoder_states_input,\n",
    "    [decoder_output] + decoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, at this point, it's important that we slow down and think about what we just constructed here. This network is not going to be *trained* in any way, and it (mostly) just reuses layers that have already created (*and* trained) in the earlier part of this example. Therefore, we are just going to rearrange some of the layers into *two* new models to make it easier to probe the network. These parts will naturally be the encoder and the decoder. However, these new models will just be used for *prediction* instead of training.\n",
    "\n",
    "The basic idea is that we need to *first* encode the input sequence into its *gestalt* context representation. This is the job of the encoder, and this part is rather straight-forward because the encoder network architecture remains practically unchanged: we just didn't give the encoder it's own, independent model before. Therefore, we could *not* just extract a hidden layer representation without also feeding it into the input layer of the decoder. The new `encoder_model` now just takes our input sequence and provides us with the output \"states\" or *activations* of the LSTM units. We can use the `predict()` method now to easily obtain those representations.\n",
    "\n",
    "The second part of the decoding process involves making a new set of inputs for the decoder which will accept the state information from the encoder outputs (in addition to the sequence starting token). This uses two layers which are just standard input layers set to the corresponding size of the LSTM outputs from the encoder. This just allows us to transfer that information over to the decoder *manually* by copying the output values from the encoder into these input layers. The hidden layer of the decoder then takes the encoder's hidden layer information and an input token vector (decoder_input, just like before).\n",
    "\n",
    "Instead of using teacher forcing to make the sequence input equal to a prescribed target token (like we did during training), here we will only provide the *start* token along with the encoder's gestalt context. After that, we will let the decoder produce an output value. This output value will then be rounded to the nearest value (it's a graded prediction, so we clean it up a little in this obvious way), and then we will feed it *back* into the decoder for the next time step (along with the updated decoder hidden layer activations as well which take the place of the encoder network's initial context information). So long as we provide the encoder's activations and the *start* token on the first step. The decoder network is then asked to step through this series of steps repeatedly until it reaches the *stop* token (or a maximum number of steps is reached). In our case, all sequences are of the same length, so we will expect it will reach the stop token by the 11th output.\n",
    "\n",
    "Let's walk through these steps by calculating the context from the encoder using our input sequence, `X`, then providing the *start* token (`0010`) to the encoder output as the first inputs to the decoder. Then we will recycle the decoder's output and hidden representations back into the network, just predicting one time step at a time until the sequence terminates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  0.,  1.,  0.]]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the gestalt context for the input sequence(s)\n",
    "context = encoder_model.predict(X)\n",
    "\n",
    "# Prep a starting token [[[0,0,1,0]]]..\n",
    "token = np.zeros([1,1,Y.shape[2]])\n",
    "token[0,0,2] = 1\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  1.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.]]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Complete 11 cycles with the decoder\n",
    "result = np.zeros(postY.shape)\n",
    "for x in range(11):\n",
    "    out,h,c = decoder_model.predict([token]+context)\n",
    "    token = np.round(out)\n",
    "    context = [h,c]\n",
    "    result[:,x,:] = token\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  1.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.]]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's compare with the pattern\n",
    "postY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we have a match! Looks like the encoder-decoder had no trouble learning the sequence. Note that we could wait for the *stop* token instead of counting the number of steps above and we would have come to the same result since that token was encountered on the last iteration. That is, the network learns to generate the right size sequence when required.\n",
    "\n",
    "OK, so now, let's take the same approach as we used above to learn a large number of sequences..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
